<center>11-08 周五 FP8和bf16 Z2000对比试验</center>

# 简介

## 天擎平台

## 文件拷贝

```bash
PS F:\fp8> mc cp -r .\llama3-dataset\ .\qwen-dataset\ nfs129/zhongziban/dataset
...tent_document.idx: 126.82 MiB / 126.82 MiB [========================================================] 8.37 MiB/s 15s
PS F:\fp8> mc cp -r .\pai-megatron-patch.tar nfs129/zhongziban
...egatron-patch.tar: 103.35 MiB / 103.35 MiB [========================================================] 7.70 MiB/s 13s
PS F:\fp8> mc cp -r .\tokenizer\ nfs129/zhongziban/model
...enizer\vocab.json: 50.64 MiB / 50.64 MiB [===========================================================] 8.24 MiB/s 6s
PS F:\fp8>                           ] 8.54 MiB/s
```

&emsp;依次将tokenizer，dataset，pai-megatron-patch传到nfs129 bucket目录下 。

## 新建算法库

### 建立llama3算法库

```bash
Pai-Megatron-Patch/examples/llama3_1
```



### 建立qwen2.5算法库

```bash
Pai-Megatron-Patch/examples/qwen2_5/
```

## 新建数据集



```bash
/mnt/nfs129/zhongziban/dataset/qwen-dataset
/mnt/nfs129/zhongziban/dataset/llama3-dataset
```

# 经验总结

1. 在none可以跑的时候，不需要再维持其他不变的基础上跑sel和full。
2. 跑70B的时候，尽量维持TP \* PP \* CP = 64，并且TP小，PP 8.
3. 8B的模型，一般4张卡可以装的下，即TP * CP * CP=4。
4. 在并行策略的时候，一般先调的是TP、 PP、 CP、这三个值变化会比较大。最后调`micro-batch-size`。

# 步骤

## 种子版之前的训练启动脚本



```bash
#!/bin/bash

set -ex

######################################
# Change the below configurations here

export CUDA_DEVICE_MAX_CONNECTIONS=1

cd  ./Megatron-LM

if [[ -z ${OUTPUT_DIR} ]];
then
  OUTPUT_DIR=output
fi

mkdir -p ${OUTPUT_DIR}

DATASET_FILE=/mnt/nfs66/datasets/Megatron-LM-main/datalist_${DATAINDEX}
DATASET="$(grep -v '^#' ${DATASET_FILE})"

CHECKPOINT_PATH="$OUTPUT_DIR/checkpoints"
mkdir -p ${CHECKPOINT_PATH}
TOKENIZER_PATH=/mnt/nfs66/datasets/Megatron-LM-main/tokenizers/qwen2

DATETIME="$(date +'%Y%m%dT%H%M%S')"
TENSORBOARD_PATH="$OUTPUT_DIR/tf_logs"

if [[ -z ${LOG_DIR} ]];
then
  LOG_DIR="$OUTPUT_DIR"
fi

TP=2
PP=1
CP=1

GPUS_PER_NODE=${TQ_GPU_NUM}
NNODES=$WORLD_SIZE

HIDDEN_SIZE=4096
FFN_HIDDEN_SIZE=11008
NUM_LAYERS=32
NUM_HEADS=32
SEQ_LENGTH=4096

MICRO_BATCH_SIZE=1
GLOBAL_BATCH_SIZE=1024
# TRAIN_STEPS=100000
LR=3e-4
MIN_LR=3e-5
LR_WARMUP_STEPS=50
WEIGHT_DECAY=0.1
GRAD_CLIP=1


EXTRA_VALID="/mnt/nfs66/datasets/pretrain-7b/valid-fixed/valid-datalist"
EXTRA_VALID_ARGS="
    --extra-valid-datalist ${EXTRA_VALID} \
    --extra-valid-data-samples $(($(python sum_row1.py ${EXTRA_VALID})/${SEQ_LENGTH})) \
    --extra-valid-data-names   "dolma-test" \
    --extra-valid-datalist ${EXTRA_VALID}-c4_en --extra-valid-data-samples $((993523/${SEQ_LENGTH})) --extra-valid-data-names c4_en \
    --extra-valid-datalist ${EXTRA_VALID}-dolma_books --extra-valid-data-samples $((486667/${SEQ_LENGTH})) --extra-valid-data-names dolma_books \
    --extra-valid-datalist ${EXTRA_VALID}-dolma_common-crawl --extra-valid-data-samples $((479817/${SEQ_LENGTH})) --extra-valid-data-names dolma_cc \
    --extra-valid-datalist ${EXTRA_VALID}-dolma_pes2o --extra-valid-data-samples $((512145/${SEQ_LENGTH})) --extra-valid-data-names dolma_pes2o \
    --extra-valid-datalist ${EXTRA_VALID}-dolma_reddit --extra-valid-data-samples $((481997/${SEQ_LENGTH})) --extra-valid-data-names dolma_reddit \
    --extra-valid-datalist ${EXTRA_VALID}-dolma_stack --extra-valid-data-samples $((416115/${SEQ_LENGTH})) --extra-valid-data-names dolma_stack \
    --extra-valid-datalist ${EXTRA_VALID}-dolma_wiki --extra-valid-data-samples $((495623/${SEQ_LENGTH})) --extra-valid-data-names dolma_wiki \
    --extra-valid-datalist ${EXTRA_VALID}-ice --extra-valid-data-samples $((896641/${SEQ_LENGTH})) --extra-valid-data-names ice \
    --extra-valid-datalist ${EXTRA_VALID}-m2d2_s2orc --extra-valid-data-samples $((979972/${SEQ_LENGTH})) --extra-valid-data-names m2d2_s2orc \
    --extra-valid-datalist ${EXTRA_VALID}-pile --extra-valid-data-samples $((666929/${SEQ_LENGTH})) --extra-valid-data-names pile \
    --extra-valid-datalist ${EXTRA_VALID}-wikitext_103 --extra-valid-data-samples $((247189/${SEQ_LENGTH})) --extra-valid-data-names wikitext_103 \
"

if [[ -z ${SEEN_STEPS} ]]; then
   SEEN_STEPS=0
fi

SAMPLE_SIZE="$(($(python3 sum_row1.py ${DATASET_FILE})*940/1000/${SEQ_LENGTH}))"
SAMPLE_ITERS="$((${SAMPLE_SIZE}/${GLOBAL_BATCH_SIZE}))"
TRAIN_STEPS=$((${SEEN_STEPS} + ${SAMPLE_ITERS}))
TOTAL_STEPS=$TRAIN_STEPS

SAVE_INTERVAL=$((${TRAIN_STEPS}/10))
EVALUATE_INTERVAL=$((${TRAIN_STEPS}/30))

echo "$( date +"%Y-%m-%dT%H:%M:%S(%Z)") - CRITICAL - Using dataset file: ${DATASET_FILE}, with ${SAMPLE_SIZE} samples" | tee -a ${OUTPUT_DIR}/data.log
echo "$( date +"%Y-%m-%dT%H:%M:%S(%Z)") - CRITICAL - Train ${SAMPLE_ITERS} steps from step $SEEN_STEPS to ${TRAIN_STEPS} with SAVE_INTERVAL ${SAVE_INTERVAL} EVALUATE_INTERVAL ${EVALUATE_INTERVAL} total $TOTAL_STEPS steps planned." | tee -a ${OUTPUT_DIR}/data.log

# # debug
#GPUS_PER_NODE=8
#NNODES=1
#MASTER_ADDR=localhost
#MASTER_PORT=12345

DISTRIBUTED_ARGS="--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES \
  --rdzv_id=333 --rdzv_backend=c10d --rdzv_endpoint $MASTER_ADDR:$MASTER_PORT \
  --tee 3 --log_dir ${LOG_DIR}/logs/${DATETIME}"

torchrun $DISTRIBUTED_ARGS \
       pretrain_gpt.py \
       --tensor-model-parallel-size $TP \
       --pipeline-model-parallel-size $PP \
       --context-parallel-size $CP\
       --num-layers $NUM_LAYERS \
       --hidden-size $HIDDEN_SIZE \
       --ffn-hidden-size $FFN_HIDDEN_SIZE \
       --num-attention-heads $NUM_HEADS \
       --micro-batch-size $MICRO_BATCH_SIZE \
       --global-batch-size $GLOBAL_BATCH_SIZE \
       --seq-length $SEQ_LENGTH \
       --max-position-embeddings $SEQ_LENGTH \
       --train-iters $TRAIN_STEPS \
       --save $CHECKPOINT_PATH \
       --load $CHECKPOINT_PATH \
       --data-path $DATASET \
       --tokenizer-type HuggingFaceTokenizer \
       --tokenizer-model $TOKENIZER_PATH \
       --vocab-file $TOKENIZER_PATH \
       --split 99,1,0 \
       --distributed-backend nccl \
       --lr $LR \
       --lr-decay-style cosine \
       --lr-decay-iters $TOTAL_STEPS \
       --min-lr $MIN_LR \
       --weight-decay $WEIGHT_DECAY \
       --clip-grad $GRAD_CLIP \
       --lr-warmup-iters $LR_WARMUP_STEPS \
       --optimizer adam \
       --adam-beta1 0.9 \
       --adam-beta2 0.95 \
       --save-interval ${SAVE_INTERVAL} \
       --log-interval 1 \
       --log-throughput \
       --log-timers-to-tensorboard \
       --log-validation-ppl-to-tensorboard \
       --timing-log-level 1 \
       --eval-interval ${EVALUATE_INTERVAL} \
       --eval-iters 1\
       --bf16\
       --attention-dropout 0 \
       --hidden-dropout 0 \
       --use-rotary-position-embeddings \
       --untie-embeddings-and-output-weights \
       --no-position-embedding \
       --swiglu \
       --normalization RMSNorm \
       --disable-bias-linear \
       --tensorboard-dir $TENSORBOARD_PATH \
       --tensorboard-log-interval 1 \
       --use-flash-attn \
       --timing-log-level 1 \
       --use-mcore-models \
       --no-masked-softmax-fusion \
       --attention-softmax-in-fp32 \
       --distributed-timeout-minutes 60 \
       --num-dataset-builder-threads 1 \
       --num-workers 2\
       --use-distributed-optimizer\
       --overlap-grad-reduce\
       --overlap-param-gather\
       --flash-save\
       --flash-load\
       --data-cache-path $OUTPUT_DIR/data_cache_$DATAINDEX 2>&1 |tee ${OUTPUT_DIR}/logs/training-log-$RANK.${DATETIME}.txt
```



## Llama3.1-8B

> 注： 请先cd到目录

```bash
Pai-Megatron-Patch/examples/llama3_1
```

### Pai-megatron-patch版本

```bash
#!/bin/bash
# please cd to the dir containing 
set -e
ENV=$1
CURRENT_DIR="$( cd "$( dirname "$0" )" && pwd )"
MEGATRON_PATH=$( dirname $( dirname ${CURRENT_DIR}))
export PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATH}/PAI-Megatron-LM-240718:$PYTHONPATH
export CUDA_DEVICE_MAX_CONNECTIONS=1
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Here are some configs controled by env
if [ -z ${MP_DATASET_TYPE} ];then
    MP_DATASET_TYPE="idxmap"
fi

if [ -z ${MP_AC_LAYERS} ];then
    MP_AC_LAYERS=1
fi

if [ $ENV = dsw ]; then
    export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
    MASTER_ADDR=localhost
    MASTER_PORT=$(shuf -n 1 -i 10000-65535)
    NNODES=1
    NODE_RANK=0
    GPUS_PER_NODE=8
elif [ $ENV = dlc ]; then
    NNODES=${WORLD_SIZE}
    NODE_RANK=${RANK}
    GPUS_PER_NODE=${KUBERNETES_CONTAINER_RESOURCE_GPU}
fi



if [ -z ${MP_SFT_PACKING} ]; then
    MP_SFT_PACKING=false
fi


DISTRIBUTED_ARGS="--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT"
EXTRA_VOCAB_SIZE=256

### BASE CONFIG ###
MODEL_SIZE=$2
BATCH_SIZE=$3
GLOBAL_BATCH_SIZE=$4
LR=$5
MIN_LR=$6
SEQ_LEN=$7
PAD_LEN=$8
PR=${9}
### BASE CONFIG ###

### PARALLEL / BOOL OPTION ###
TP=${10}
PP=${11}
CP=${12}
SP=${13}
DO=${14}
FL=${15}
SFT=${16}
### PARALLEL / BOOL OPTION ###

### OTHERS ###
AC=${17}
OPTIMIZER_OFFLOAD=${18}
SAVE_INTERVAL=${19}
DATASET_PATH=${20}
VALID_DATASET_PATH=${21}
PRETRAIN_CHECKPOINT_PATH=${22}

# the following two values will not be used when SFT is true
TRAIN_TOKENS=${23}
WARMUP_TOKENS=${24}
###############################

OUTPUT_BASEPATH=${25}
### OTHERS ###

if [ $FL = true ]; then
    export NVTE_FLASH_ATTN=1 NVTE_FUSED_ATTN=0
elif [ $FL = false ]; then
    export NVTE_FLASH_ATTN=0 NVTE_FUSED_ATTN=1
fi

if [ $MODEL_SIZE = 8B ]; then

NUM_LAYERS=32
HIDDEN_SIZE=4096
NUM_ATTN_HEADS=32
INTERMEDIATE_SIZE=14336
NUM_KEY_VALUE_HEADS=8
MAX_POSITION_EMBEDDINGS=131072

gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"


elif [ $MODEL_SIZE = 70B ]; then

NUM_LAYERS=80
HIDDEN_SIZE=8192
NUM_ATTN_HEADS=64
INTERMEDIATE_SIZE=28672
NUM_KEY_VALUE_HEADS=8
MAX_POSITION_EMBEDDINGS=131072
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

fi

MP_VP=$(( $NUM_LAYERS / $PP / 4 ))
if [ -z ${MP_VP} ]; then
    vp_options=""
else
    vp_options=" \
        --num-layers-per-virtual-pipeline-stage ${MP_VP}"
fi

TP_COMM_OVERLAP=$(( ($TP > 1) ? 1 : 0 ))

comm_overlap_option="\
    --overlap-grad-reduce \
    --overlap-param-gather"

if [ $TP_COMM_OVERLAP -eq 1 ]; then
    comm_overlap_option="\
        --tp-comm-overlap \
        --overlap-grad-reduce \
        --overlap-param-gather"
fi

if [ $AC = full ]; then
    _check=$(( ($NUM_LAYERS / $PP) % ${MP_AC_LAYERS} ))
    if [ $_check != 0 ]; then
        echo "the num layers per pp rank must be a multiple of the recompute layers."
        exit -1
    fi
    activation_checkpoint_options=" \
                    --recompute-method uniform \
            --recompute-num-layers ${MP_AC_LAYERS} \
                    --recompute-granularity full"
elif [ $AC = sel ]; then
    activation_checkpoint_options=" \
        --recompute-activations"
elif [ $AC = none ]; then
    activation_checkpoint_options=" \
    "
elif [ $AC = offload ]; then
    activation_checkpoint_options=" \
                    --cpu-offloading \
                    --cpu-offloading-num-layers ${MP_AC_LAYERS}"
    if [ $TP_COMM_OVERLAP -eq 1 ]; then
        echo "Disable --overlap-grad-reduce and --overlap-param-gather when cpu offloading is on..."
        comm_overlap_option="\
            --tp-comm-overlap"
    else
        echo "Disable --overlap-grad-reduce and --overlap-param-gather when cpu offloading is on..."
        comm_overlap_option=""
    fi
fi

if [ $PR = fp16 ]; then
    pr_options=" \
                    --fp16 \
            --apply-query-key-layer-scaling"
    export NVTE_APPLY_QK_LAYER_SCALING=1
elif [ $PR = bf16 ]; then
    pr_options=" \
        --bf16"
elif [ $PR = fp8 ]; then
    pr_options=" \
        --bf16 \
        --fp8-format hybrid \
        --fp8-amax-compute-algo max \
        --fp8-amax-history-len 1024"
fi

if [ $OPTIMIZER_OFFLOAD != false ] && [ $DO = false ]; then
    echo "Offload optimizer is valid only if \$DO=true"
    DO=true
fi

if [ $DO = true ]; then
    do_options=" \
                    --use-distributed-optimizer"

elif [ $DO = false ]; then
    do_options=" \
                    "
fi

te_options=" \
        --transformer-impl transformer_engine"

if [ $SP = true ] && [ $TP -gt 1 ]; then
    sp_options=" \
                    --sequence-parallel"

elif [ $SP = false ]; then
    sp_options=" \
                    "
fi

if [ $PRETRAIN_CHECKPOINT_PATH != none ]; then
    load_options=" \
            --load $PRETRAIN_CHECKPOINT_PATH"
fi

if [ $OPTIMIZER_OFFLOAD = 'static' ]; then
    offload_option=" \
        --optimizer hybridadam \
        --optimizer-offload-policy static \
        --optimizer-offload-fraction 1.0"
elif [ $OPTIMIZER_OFFLOAD = 'auto' ]; then
    offload_option=" \
        --optimizer hybridadam \
        --optimizer-offload-policy auto"
else
    offload_option=""
fi

if [ $SFT = true ]; then
    TRAIN_ITERS=${23}
    LR_WARMUP_ITERS=${24}
    LR_DECAY_ITERS=$(( ${TRAIN_ITERS} - ${LR_WARMUP_ITERS}))
    PREFIX="finetune-mcore-llama3-1-${MODEL_SIZE}-lr-${LR}-minlr-${MIN_LR}-bs-${BATCH_SIZE}-gbs-${GLOBAL_BATCH_SIZE}-seqlen-${SEQ_LEN}"
    sft_option=" \
         --eod-mask-loss \
         --train-mode finetune"
else
    TRAIN_ITERS=$(( ${TRAIN_TOKENS} / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    LR_WARMUP_ITERS=$(( ${WARMUP_TOKENS}  / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    LR_DECAY_ITERS=$(( ${TRAIN_TOKENS} /  ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    PREFIX="pretrain-mcore-llama3-1-${MODEL_SIZE}-lr-${LR}-minlr-${MIN_LR}-bs-${BATCH_SIZE}-gbs-${GLOBAL_BATCH_SIZE}-seqlen-${SEQ_LEN}"
    sft_option=" \
        --train-mode pretrain"
fi

if [ ${MP_DATASET_TYPE} = "raw" ]; then
    dataset_option=" \
        --train-data-path ${DATASET_PATH} \
        --valid-data-path ${VALID_DATASET_PATH} \
        --dataloader-type cyclic \
        --dataset LLama-SFT-Raw"
else 
    dataset_option=" \
        --data-path ${DATASET_PATH} \
        --split 99,1,0 \
        --dataset LLama-Pretrain-Idxmap"
fi

if [ ${MP_SFT_PACKING} = true ]; then
    packing_options=" \
        --reset-position-ids \
        --no-create-attention-mask-in-dataloader
    "
else
    packing_options=""
fi


##### Prepare logdirs #######
NAME="${PREFIX}-pr-${PR}-tp-${TP}-pp-${PP}-cp-${CP}-ac-${AC}-do-${DO}-sp-${SP}-ti-${TRAIN_ITERS}-wi-${LR_WARMUP_ITERS}"
mkdir -p "${OUTPUT_BASEPATH}/tensorboard/"
mkdir -p "${OUTPUT_BASEPATH}/checkpoint/"
mkdir -p "${OUTPUT_BASEPATH}/log/"
current_time=$(date "+%Y.%m.%d-%H.%M.%S")
TENSORBOARD_DIR="${OUTPUT_BASEPATH}/tf_logs"
mkdir -p ${TENSORBOARD_DIR}
SAVED_PRETRAIN_CHECKPOINT_PATH="${OUTPUT_BASEPATH}/checkpoint/${NAME}"

mkdir -p ${SAVED_PRETRAIN_CHECKPOINT_PATH}
find -L ${PRETRAIN_CHECKPOINT_PATH} -maxdepth 1 -type f -name "*.json" -print0 | xargs -0 cp -t ${SAVED_PRETRAIN_CHECKPOINT_PATH}


megatron_options="  \
        --save ${SAVED_PRETRAIN_CHECKPOINT_PATH} \
        --lr ${LR} \
        --min-lr ${MIN_LR} \
        --lr-decay-style cosine \
        --weight-decay 0.1 \
        --adam-beta1 0.9 \
        --adam-beta2 0.95 \
        --clip-grad 1.0 \
        --init-method-std 0.008 \
        --attention-dropout 0.0 \
        --hidden-dropout 0.0 \
        --lr-decay-iters ${LR_DECAY_ITERS} \
        --lr-warmup-iters ${LR_WARMUP_ITERS} \
        --train-iters ${TRAIN_ITERS} \
        --micro-batch-size ${BATCH_SIZE} \
        --global-batch-size ${GLOBAL_BATCH_SIZE} \
        --num-layers ${NUM_LAYERS} \
        --hidden-size ${HIDDEN_SIZE} \
        --num-attention-heads ${NUM_ATTN_HEADS} \
        --ffn-hidden-size ${INTERMEDIATE_SIZE} \
        --seq-length ${SEQ_LEN} \
        --max-position-embeddings ${MAX_POSITION_EMBEDDINGS} \
        --max-padding-length ${PAD_LEN} \
        --log-interval 1 \
        --log-throughput \
        --eval-interval 10000 \
        --eval-iters 10 \
        --save-interval ${SAVE_INTERVAL} \
        --tensorboard-queue-size 1 \
        --tensorboard-dir ${TENSORBOARD_DIR} \
        --log-timers-to-tensorboard \
        --log-batch-size-to-tensorboard \
        --log-validation-ppl-to-tensorboard \
        --tensor-model-parallel-size ${TP} \
        --pipeline-model-parallel-size ${PP} \
        --context-parallel-size ${CP} \
        --no-load-optim \
        --no-load-rng \
        --num-workers 8 \
        --extra-vocab-size ${EXTRA_VOCAB_SIZE} \
        --patch-tokenizer-type LLama3Tokenizer \
        --swiglu \
        --normalization RMSNorm \
        --norm-epsilon 1e-05 \
        --use-rotary-position-embeddings \
        --position-embedding-type rope \
        --untie-embeddings-and-output-weights \
        --disable-bias-linear \
        --rotary-base 500000 \
        --no-save-optim \
        "

run_cmd="torchrun $DISTRIBUTED_ARGS pretrain_llama.py
 ${megatron_options} ${dataset_option} ${pr_options} ${load_options} ${te_options} ${activation_checkpoint_options} \
 ${do_options} ${sp_options} ${gqa_options} ${offload_option} ${comm_overlap_option} ${sft_option} ${vp_options} ${packing_options}"

echo ${run_cmd}
eval ${run_cmd}
set +x
```

### 天擎-Z2000版本

可以使用的环境变量：

```bash
OUTPUT_DIR=/mnt/nfs129/zhongziban/output/llama3/abc123ab
PWD=/mnt/nfs129/zhongziban
```



```bash
#!/bin/bash
# please cd to the dir containing 
set -e



pwd
env
OUTPUT_BASEPATH="$OUTPUT_DIR"
DATETIME="$(date +'%Y%m%dT%H%M%S')"
# 使用huggingface模型，则会随机进行初始化
PRETRAIN_CHECKPOINT_PATH="$(pwd)"/model/Meta-Llama-3.1-8B-tokenizer/

cd Pai-Megatron-Patch/examples/llama3_1
ENV=dlc
CURRENT_DIR="$( cd "$( dirname "$0" )" && pwd )"
MEGATRON_PATH=$( dirname $( dirname ${CURRENT_DIR}))

LOG_DIR="${OUTPUT_BASEPATH}"
export PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATH}/PAI-Megatron-LM-240718:$PYTHONPATH
export CUDA_DEVICE_MAX_CONNECTIONS=1
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
echo "CURRENT_DIR: ${CURRENT_DIR}"
echo "MEGATRON_PATH: ${MEGATRON_PATH}"
# Here are some configs controled by env
if [ -z ${MP_DATASET_TYPE} ];then
    MP_DATASET_TYPE="idxmap"
fi

if [ -z ${MP_AC_LAYERS} ];then
    MP_AC_LAYERS=1
fi

if [ $ENV = dsw ]; then
    export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
    MASTER_ADDR=localhost
    MASTER_PORT=$(shuf -n 1 -i 10000-65535)
    NNODES=1
    NODE_RANK=0
    GPUS_PER_NODE=8
elif [ $ENV = dlc ]; then
    NNODES=${WORLD_SIZE}
    GPUS_PER_NODE=${TQ_GPU_NUM}
fi

if [ -z ${MP_VP} ]; then
    vp_options=""
else
    vp_options=" \
        --num-layers-per-virtual-pipeline-stage ${MP_VP}"
fi

if [ -z ${MP_SFT_PACKING} ]; then
    MP_SFT_PACKING=false
fi


DISTRIBUTED_ARGS="--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES \
  --rdzv_id=333 --rdzv_backend=c10d --rdzv_endpoint $MASTER_ADDR:$MASTER_PORT \
  --tee 3 --log_dir ${LOG_DIR}/logs/${DATETIME}"

EXTRA_VOCAB_SIZE=256

### BASE CONFIG ###
MODEL_SIZE=8B
BATCH_SIZE=1
GLOBAL_BATCH_SIZE=1024
LR=1e-5
MIN_LR=1e-6
SEQ_LEN=8192
PAD_LEN=8192
PR=bf16
### BASE CONFIG ###

### PARALLEL / BOOL OPTION ###
TP=1
PP=1
CP=1
SP=true
DO=true
FL=true
SFT=false
### PARALLEL / BOOL OPTION ###

### OTHERS ###
AC=false
OPTIMIZER_OFFLOAD=false
SAVE_INTERVAL=100000
DATASET_PATH=/mnt/nfs129/zhongziban/dataset/llama3-dataset/wudao_llama3bpe_content_document
VALID_DATASET_PATH=/mnt/nfs129/zhongziban/dataset/llama3-dataset/wudao_llama3bpe_content_document


# the following two values will not be used when SFT is true
TRAIN_TOKENS=100000000000
WARMUP_TOKENS=100000000
###############################


### OTHERS ###

if [ $FL = true ]; then
    export NVTE_FLASH_ATTN=1 NVTE_FUSED_ATTN=0
elif [ $FL = false ]; then
    export NVTE_FLASH_ATTN=0 NVTE_FUSED_ATTN=1
fi

if [ $MODEL_SIZE = 8B ]; then

NUM_LAYERS=32
HIDDEN_SIZE=4096
NUM_ATTN_HEADS=32
INTERMEDIATE_SIZE=14336
NUM_KEY_VALUE_HEADS=8
MAX_POSITION_EMBEDDINGS=131072

gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"


elif [ $MODEL_SIZE = 70B ]; then

NUM_LAYERS=80
HIDDEN_SIZE=8192
NUM_ATTN_HEADS=64
INTERMEDIATE_SIZE=28672
NUM_KEY_VALUE_HEADS=8
MAX_POSITION_EMBEDDINGS=131072
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

fi

TP_COMM_OVERLAP=$(( ($TP > 1) ? 1 : 0 ))

comm_overlap_option="\
    --overlap-grad-reduce \
    --overlap-param-gather"

if [ $TP_COMM_OVERLAP -eq 1 ]; then
    comm_overlap_option="\
        --tp-comm-overlap \
        --overlap-grad-reduce \
        --overlap-param-gather"
fi

if [ $AC = full ]; then
    _check=$(( ($NUM_LAYERS / $PP) % ${MP_AC_LAYERS} ))
    if [ $_check != 0 ]; then
        echo "the num layers per pp rank must be a multiple of the recompute layers."
        exit -1
    fi
    activation_checkpoint_options=" \
                    --recompute-method uniform \
            --recompute-num-layers ${MP_AC_LAYERS} \
                    --recompute-granularity full"
elif [ $AC = sel ]; then
    activation_checkpoint_options=" \
        --recompute-activations"
elif [ $AC = none ]; then
    activation_checkpoint_options=" \
    "
elif [ $AC = offload ]; then
    activation_checkpoint_options=" \
                    --cpu-offloading \
                    --cpu-offloading-num-layers ${MP_AC_LAYERS}"
    if [ $TP_COMM_OVERLAP -eq 1 ]; then
        echo "Disable --overlap-grad-reduce and --overlap-param-gather when cpu offloading is on..."
        comm_overlap_option="\
            --tp-comm-overlap"
    else
        echo "Disable --overlap-grad-reduce and --overlap-param-gather when cpu offloading is on..."
        comm_overlap_option=""
    fi
fi

if [ $PR = fp16 ]; then
    pr_options=" \
                    --fp16 \
            --apply-query-key-layer-scaling"
    export NVTE_APPLY_QK_LAYER_SCALING=1
elif [ $PR = bf16 ]; then
    pr_options=" \
        --bf16"
elif [ $PR = fp8 ]; then
    pr_options=" \
        --bf16 \
        --fp8-format hybrid \
        --fp8-amax-compute-algo max \
        --fp8-amax-history-len 1024"
fi

if [ $OPTIMIZER_OFFLOAD != false ] && [ $DO = false ]; then
    echo "Offload optimizer is valid only if \$DO=true"
    DO=true
fi

if [ $DO = true ]; then
    do_options=" \
                    --use-distributed-optimizer"

elif [ $DO = false ]; then
    do_options=" \
                    "
fi

te_options=" \
        --transformer-impl transformer_engine"

if [ $SP = true ] && [ $TP -gt 1 ]; then
    sp_options=" \
                    --sequence-parallel"

elif [ $SP = false ]; then
    sp_options=" \
                    "
fi

if [ $PRETRAIN_CHECKPOINT_PATH != none ]; then
    load_options=" \
            --load $PRETRAIN_CHECKPOINT_PATH"
fi

if [ $OPTIMIZER_OFFLOAD = 'static' ]; then
    offload_option=" \
        --optimizer hybridadam \
        --optimizer-offload-policy static \
        --optimizer-offload-fraction 1.0"
elif [ $OPTIMIZER_OFFLOAD = 'auto' ]; then
    offload_option=" \
        --optimizer hybridadam \
        --optimizer-offload-policy auto"
else
    offload_option=""
fi

if [ $SFT = true ]; then
    TRAIN_ITERS=${23}
    LR_WARMUP_ITERS=${24}
    LR_DECAY_ITERS=$(( ${TRAIN_ITERS} - ${LR_WARMUP_ITERS}))
    PREFIX="finetune-mcore-llama3-1-${MODEL_SIZE}-lr-${LR}-minlr-${MIN_LR}-bs-${BATCH_SIZE}-gbs-${GLOBAL_BATCH_SIZE}-seqlen-${SEQ_LEN}"
    sft_option=" \
         --eod-mask-loss \
         --train-mode finetune"
else
    TRAIN_ITERS=$(( ${TRAIN_TOKENS} / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    LR_WARMUP_ITERS=$(( ${WARMUP_TOKENS}  / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    LR_DECAY_ITERS=$(( ${TRAIN_TOKENS} /  ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    PREFIX="pretrain-mcore-llama3-1-${MODEL_SIZE}-lr-${LR}-minlr-${MIN_LR}-bs-${BATCH_SIZE}-gbs-${GLOBAL_BATCH_SIZE}-seqlen-${SEQ_LEN}"
    sft_option=" \
        --train-mode pretrain"
fi

if [ ${MP_DATASET_TYPE} = "raw" ]; then
    dataset_option=" \
        --train-data-path ${DATASET_PATH} \
        --valid-data-path ${VALID_DATASET_PATH} \
        --dataloader-type cyclic \
        --dataset LLama-SFT-Raw"
else 
    dataset_option=" \
        --data-path ${DATASET_PATH} \
        --split 99,1,0 \
        --dataset LLama-Pretrain-Idxmap"
fi

if [ ${MP_SFT_PACKING} = true ]; then
    packing_options=" \
        --reset-position-ids \
        --no-create-attention-mask-in-dataloader
    "
else
    packing_options=""
fi


##### Prepare logdirs #######
NAME="${PREFIX}-pr-${PR}-tp-${TP}-pp-${PP}-cp-${CP}-ac-${AC}-do-${DO}-sp-${SP}-ti-${TRAIN_ITERS}-wi-${LR_WARMUP_ITERS}"
mkdir -p "${OUTPUT_BASEPATH}/tensorboard/"
mkdir -p "${OUTPUT_BASEPATH}/checkpoint/"
mkdir -p "${OUTPUT_BASEPATH}/log/"
current_time=$(date "+%Y.%m.%d-%H.%M.%S")
TENSORBOARD_DIR="${OUTPUT_BASEPATH}/tensorboard/${NAME}_${current_time}"
mkdir -p ${TENSORBOARD_DIR}
SAVED_PRETRAIN_CHECKPOINT_PATH="${OUTPUT_BASEPATH}/checkpoint/${NAME}"

mkdir -p ${SAVED_PRETRAIN_CHECKPOINT_PATH}
find -L ${PRETRAIN_CHECKPOINT_PATH} -maxdepth 1 -type f -name "*.json" -print0 | xargs -0 cp -t ${SAVED_PRETRAIN_CHECKPOINT_PATH}


megatron_options="  \
        --save ${SAVED_PRETRAIN_CHECKPOINT_PATH} \
        --lr ${LR} \
        --min-lr ${MIN_LR} \
        --lr-decay-style cosine \
        --weight-decay 0.1 \
        --adam-beta1 0.9 \
        --adam-beta2 0.95 \
        --clip-grad 1.0 \
        --init-method-std 0.008 \
        --attention-dropout 0.0 \
        --hidden-dropout 0.0 \
        --lr-decay-iters ${LR_DECAY_ITERS} \
        --lr-warmup-iters ${LR_WARMUP_ITERS} \
        --train-iters ${TRAIN_ITERS} \
        --micro-batch-size ${BATCH_SIZE} \
        --global-batch-size ${GLOBAL_BATCH_SIZE} \
        --num-layers ${NUM_LAYERS} \
        --hidden-size ${HIDDEN_SIZE} \
        --num-attention-heads ${NUM_ATTN_HEADS} \
        --ffn-hidden-size ${INTERMEDIATE_SIZE} \
        --seq-length ${SEQ_LEN} \
        --max-position-embeddings ${MAX_POSITION_EMBEDDINGS} \
        --max-padding-length ${PAD_LEN} \
        --log-interval 1 \
        --log-throughput \
        --eval-interval 10000 \
        --eval-iters 10 \
        --save-interval ${SAVE_INTERVAL} \
        --tensorboard-queue-size 1 \
        --tensorboard-dir ${TENSORBOARD_DIR} \
        --log-timers-to-tensorboard \
        --log-batch-size-to-tensorboard \
        --log-validation-ppl-to-tensorboard \
        --tensor-model-parallel-size ${TP} \
        --pipeline-model-parallel-size ${PP} \
        --context-parallel-size ${CP} \
        --no-load-optim \
        --no-load-rng \
        --num-workers 8 \
        --extra-vocab-size ${EXTRA_VOCAB_SIZE} \
        --patch-tokenizer-type LLama3Tokenizer \
        --swiglu \
        --normalization RMSNorm \
        --norm-epsilon 1e-05 \
        --use-rotary-position-embeddings \
        --position-embedding-type rope \
        --untie-embeddings-and-output-weights \
        --disable-bias-linear \
        --rotary-base 500000 \
        --no-save-optim \
        "

run_cmd="torchrun $DISTRIBUTED_ARGS pretrain_llama.py
 ${megatron_options} ${dataset_option} ${pr_options} ${load_options} ${te_options} ${activation_checkpoint_options} \
 ${do_options} ${sp_options} ${gqa_options} ${offload_option} ${comm_overlap_option} ${sft_option} ${vp_options} ${packing_options} 2>&1 |tee ${OUTPUT_DIR}/logs/training-log-$RANK.${DATETIME}.txt"

echo ${run_cmd}
eval ${run_cmd}
set +x
```



```bash
CURRENT_DIR: /mnt/nfs129/zhongziban/Pai-Megatron-Patch/examples/llama3_1
MEGATRON_PATH: /mnt/nfs129/zhongziban/Pai-Megatron-Patch
torchrun --nproc_per_node 8 --nnodes 4 --rdzv_id=333 --rdzv_backend=c10d --rdzv_endpoint localhost:24896 --tee 3 --log_dir /mnt/nfs129/zhongziban/output/llama3/llama3-bf16-TP1-PP4-CP1/logs/20241111T095442 pretrain_llama.py --save /mnt/nfs129/zhongziban/output/llama3/llama3-bf16-TP1-PP4-CP1/checkpoint/pretrain-mcore-llama3-1-8B-lr-1e-5-minlr-1e-6-bs-1-gbs-1024-seqlen-8192-pr-fp8-tp-2-pp-2-cp-1-ac-sel-do-true-sp-true-ti-11920-wi-11 --lr 1e-5 --min-lr 1e-6 --lr-decay-style cosine --weight-decay 0.1 --adam-beta1 0.9 --adam-beta2 0.95 --clip-grad 1.0 --init-method-std 0.008 --attention-dropout 0.0 --hidden-dropout 0.0 --lr-decay-iters 11920 --lr-warmup-iters 11 --train-iters 11920 --micro-batch-size 1 --global-batch-size 1024 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --ffn-hidden-size 14336 --seq-length 8192 --max-position-embeddings 131072 --max-padding-length 8192 --log-interval 1 --log-throughput --eval-interval 10000 --eval-iters 10 --save-interval 100000 --tensorboard-queue-size 1 --tensorboard-dir /mnt/nfs129/zhongziban/output/llama3/llama3-bf16-TP1-PP4-CP1/tensorboard/pretrain-mcore-llama3-1-8B-lr-1e-5-minlr-1e-6-bs-1-gbs-1024-seqlen-8192-pr-fp8-tp-2-pp-2-cp-1-ac-sel-do-true-sp-true-ti-11920-wi-11_2024.11.11-09.54.42 --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensor-model-parallel-size 2 --pipeline-model-parallel-size 2 --context-parallel-size 1 --no-load-optim --no-load-rng --num-workers 8 --extra-vocab-size 256 --patch-tokenizer-type LLama3Tokenizer --swiglu --normalization RMSNorm --norm-epsilon 1e-05 --use-rotary-position-embeddings --position-embedding-type rope --untie-embeddings-and-output-weights --disable-bias-linear --rotary-base 500000 --no-save-optim --data-path /mnt/nfs129/zhongziban/dataset/llama3-dataset/wudao_llama3bpe_content_document --split 99,1,0 --dataset LLama-Pretrain-Idxmap --bf16 --fp8-format hybrid --fp8-amax-compute-algo max --fp8-amax-history-len 1024 --load /mnt/nfs129/zhongziban/model/Meta-Llama-3.1-8B-tokenizer/ --transformer-impl transformer_engine --recompute-activations --use-distributed-optimizer --sequence-parallel --group-query-attention --num-query-groups 8 --tp-comm-overlap --overlap-grad-reduce --overlap-param-gather --train-mode pretrain 2>&1 |tee /mnt/nfs129/zhongziban/output/llama3/llama3-bf16-TP1-PP4-CP1/logs/training-log-0.20241111T095442.txt
W1111 09:54:43.593000 140067174844224 torch/distributed/run.py:778] 
W1111 09:54:43.593000 140067174844224 torch/distributed/run.py:778] *****************************************
W1111 09:54:43.593000 140067174844224 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1111 09:54:43.593000 140067174844224 torch/distributed/run.py:778] *****************************************
```



**model	size	precision	sequence length**	TP	PP	CP	**micro batch size	global batch size**	activation_checkpoint_options	num-layers-per-virtual-pipeline-stage	throughput per GPU (TFLOP/s/GPU)	MFU(%)	单卡TGS	elapsed time per iteration (ms)	max memory cost(GB)



### 采样数据-LLama3-8B-4机

对于**LLama3.1 8B**的模型，精度采用bf16和FP8 micro_batch_size为1，而global batch_size为1024，进行如下的采样：

| precision | TP   | PP   | CP   | AC      | num-layers-per-<br />virtual-pipeline-stage | TFLOPS | elapsed time | memory                                                       |
| --------- | ---- | ---- | ---- | ------- | ------------------------------------------- | ------ | ------------ | ------------------------------------------------------------ |
| bf16      | 4    | 2    | 1    | false   |                                             | 418.0  | 72633.6      | 25050MiB                                                     |
| bf16      | 1    | 1    | 1    | false   |                                             |        |              | OOM                                                          |
| bf16      | 2    | 1    | 1    | false   |                                             |        |              | OOM                                                          |
| bf16      | 4    | 1    | 1    | false   |                                             |        |              | OOM                                                          |
| bf16      | 2    | 2    | 1    | false   |                                             | 459.1  | 33070.9      | 31458MiB                                                     |
| bf16      | 1    | 1    | 4    | false   |                                             | 443.4  | 34242.5      | 55762MiB                                                     |
| fp8       | 2    | 2    | 1    | false   |                                             | 582.4  | 26068.1      | 34000MiB                                                     |
| fp8       | 1    | 4    | 1    | sel     |                                             | 550.9  | 27558.8      | 38722MiB                                                     |
| fp8       | 1    | 2    | 1    | sel     |                                             | 607.2  | 25003.0      | 67710MiB                                                     |
| bf16      | 1    | 2    | 1    | false   |                                             | 501.5  | 30272.5      | 65012MiB                                                     |
| fp8       | 1    | 4    | 1    | false   |                                             | 548.2  | 27695.9      | 54416MiB                                                     |
| fp8       | 2    | 4    | 1    | false   |                                             | 499.0  | 30425.7      | 32318MiB                                                     |
| fp8       | 1    | 2    | 1    | sel     | 4                                           | 603.8  | 25143.4      | 73326MiB                                                     |
| fp8       | 1    | 2    | 1    | full    | 4                                           |        |              | TypeError: te_checkpoint() missing 1 required positional argument: 'packed_seq_params'<br />torch.distributed.DistBackendError: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0:1', but store->get('0:1') got error: Connection reset by peer<br/>[default4]:[rank20]: Exception raised from recvBytes at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/Utils.hpp:672 (most recent call first): |
| fp8       | 1    | 2    | 1    | offload | 4                                           |        |              | ValueError: Currently there is no support for Pipeline parallelism with CPU offloading |
| fp8       | 1    | 2    | 1    | false   | 4                                           | 644.1  | 23570.5      | 72542MiB                                                     |
| fp8       | 1    | 2    | 2    | false   | 4                                           | 602.1  | 25214.1      | 56506MiB                                                     |
| bf16      | 1    | 2    | 1    | false   | 4                                           | 501.7  | 30259.4      | 71150MiB                                                     |



### fp8-tp1-pp2-ac-sel-virtual4

```bash
#!/bin/bash
# please cd to the dir containing 
set -e
OUTPUT_BASEPATH="$OUTPUT_DIR"
DATETIME="$(date +'%Y%m%dT%H%M%S')"
# 使用huggingface模型，则会随机进行初始化
PRETRAIN_CHECKPOINT_PATH="$(pwd)"/model/Meta-Llama-3.1-8B-tokenizer/

cd Pai-Megatron-Patch/examples/llama3_1
ENV=dlc
CURRENT_DIR="$( cd "$( dirname "$0" )" && pwd )"
MEGATRON_PATH=$( dirname $( dirname ${CURRENT_DIR}))

LOG_DIR="${OUTPUT_BASEPATH}"
export PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATH}/PAI-Megatron-LM-240718:$PYTHONPATH
export CUDA_DEVICE_MAX_CONNECTIONS=1
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
echo "CURRENT_DIR: ${CURRENT_DIR}"
echo "MEGATRON_PATH: ${MEGATRON_PATH}"
# Here are some configs controled by env
if [ -z ${MP_DATASET_TYPE} ];then
    MP_DATASET_TYPE="idxmap"
fi

if [ -z ${MP_AC_LAYERS} ];then
    MP_AC_LAYERS=1
fi

if [ $ENV = dsw ]; then
    export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
    MASTER_ADDR=localhost
    MASTER_PORT=$(shuf -n 1 -i 10000-65535)
    NNODES=1
    NODE_RANK=0
    GPUS_PER_NODE=8
elif [ $ENV = dlc ]; then
    NNODES=${WORLD_SIZE}
    GPUS_PER_NODE=${TQ_GPU_NUM}
fi



if [ -z ${MP_SFT_PACKING} ]; then
    MP_SFT_PACKING=false
fi


DISTRIBUTED_ARGS="--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES \
  --rdzv_id=333 --rdzv_backend=c10d --rdzv_endpoint $MASTER_ADDR:$MASTER_PORT \
  --tee 3 --log_dir ${LOG_DIR}/logs/${DATETIME}"

EXTRA_VOCAB_SIZE=256

### BASE CONFIG ###
MODEL_SIZE=8B
BATCH_SIZE=1
GLOBAL_BATCH_SIZE=1024
LR=1e-5
MIN_LR=1e-6
SEQ_LEN=8192
PAD_LEN=8192
PR=fp8
### BASE CONFIG ###

### PARALLEL / BOOL OPTION ###
TP=1
PP=2
CP=1
SP=true
DO=true
FL=true
SFT=false
### PARALLEL / BOOL OPTION ###

### OTHERS ###
AC=sel
OPTIMIZER_OFFLOAD=false
SAVE_INTERVAL=100000
DATASET_PATH=/mnt/nfs129/zhongziban/dataset/llama3-dataset/wudao_llama3bpe_content_document
VALID_DATASET_PATH=/mnt/nfs129/zhongziban/dataset/llama3-dataset/wudao_llama3bpe_content_document


# the following two values will not be used when SFT is true
TRAIN_TOKENS=100000000000
WARMUP_TOKENS=100000000
###############################


### OTHERS ###

if [ $FL = true ]; then
    export NVTE_FLASH_ATTN=1 NVTE_FUSED_ATTN=0
elif [ $FL = false ]; then
    export NVTE_FLASH_ATTN=0 NVTE_FUSED_ATTN=1
fi

if [ $MODEL_SIZE = 8B ]; then

NUM_LAYERS=32
HIDDEN_SIZE=4096
NUM_ATTN_HEADS=32
INTERMEDIATE_SIZE=14336
NUM_KEY_VALUE_HEADS=8
MAX_POSITION_EMBEDDINGS=131072

gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"


elif [ $MODEL_SIZE = 70B ]; then

NUM_LAYERS=80
HIDDEN_SIZE=8192
NUM_ATTN_HEADS=64
INTERMEDIATE_SIZE=28672
NUM_KEY_VALUE_HEADS=8
MAX_POSITION_EMBEDDINGS=131072
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

fi

MP_VP=$(( $NUM_LAYERS / $PP / 4 ))

if [ -z ${MP_VP} ]; then
    vp_options=""
else
    vp_options=" \
        --num-layers-per-virtual-pipeline-stage ${MP_VP}"
fi


TP_COMM_OVERLAP=$(( ($TP > 1) ? 1 : 0 ))

comm_overlap_option="\
    --overlap-grad-reduce \
    --overlap-param-gather"

if [ $TP_COMM_OVERLAP -eq 1 ]; then
    comm_overlap_option="\
        --tp-comm-overlap \
        --overlap-grad-reduce \
        --overlap-param-gather"
fi

if [ $AC = full ]; then
    _check=$(( ($NUM_LAYERS / $PP) % ${MP_AC_LAYERS} ))
    if [ $_check != 0 ]; then
        echo "the num layers per pp rank must be a multiple of the recompute layers."
        exit -1
    fi
    activation_checkpoint_options=" \
                    --recompute-method uniform \
            --recompute-num-layers ${MP_AC_LAYERS} \
                    --recompute-granularity full"
elif [ $AC = sel ]; then
    activation_checkpoint_options=" \
        --recompute-activations"
elif [ $AC = none ]; then
    activation_checkpoint_options=" \
    "
elif [ $AC = offload ]; then
    activation_checkpoint_options=" \
                    --cpu-offloading \
                    --cpu-offloading-num-layers ${MP_AC_LAYERS}"
    if [ $TP_COMM_OVERLAP -eq 1 ]; then
        echo "Disable --overlap-grad-reduce and --overlap-param-gather when cpu offloading is on..."
        comm_overlap_option="\
            --tp-comm-overlap"
    else
        echo "Disable --overlap-grad-reduce and --overlap-param-gather when cpu offloading is on..."
        comm_overlap_option=""
    fi
fi

if [ $PR = fp16 ]; then
    pr_options=" \
                    --fp16 \
            --apply-query-key-layer-scaling"
    export NVTE_APPLY_QK_LAYER_SCALING=1
elif [ $PR = bf16 ]; then
    pr_options=" \
        --bf16"
elif [ $PR = fp8 ]; then
    pr_options=" \
        --bf16 \
        --fp8-format hybrid \
        --fp8-amax-compute-algo max \
        --fp8-amax-history-len 1024"
fi

if [ $OPTIMIZER_OFFLOAD != false ] && [ $DO = false ]; then
    echo "Offload optimizer is valid only if \$DO=true"
    DO=true
fi

if [ $DO = true ]; then
    do_options=" \
                    --use-distributed-optimizer"

elif [ $DO = false ]; then
    do_options=" \
                    "
fi

te_options=" \
        --transformer-impl transformer_engine"

if [ $SP = true ] && [ $TP -gt 1 ]; then
    sp_options=" \
                    --sequence-parallel"

elif [ $SP = false ]; then
    sp_options=" \
                    "
fi

if [ $PRETRAIN_CHECKPOINT_PATH != none ]; then
    load_options=" \
            --load $PRETRAIN_CHECKPOINT_PATH"
fi

if [ $OPTIMIZER_OFFLOAD = 'static' ]; then
    offload_option=" \
        --optimizer hybridadam \
        --optimizer-offload-policy static \
        --optimizer-offload-fraction 1.0"
elif [ $OPTIMIZER_OFFLOAD = 'auto' ]; then
    offload_option=" \
        --optimizer hybridadam \
        --optimizer-offload-policy auto"
else
    offload_option=""
fi

if [ $SFT = true ]; then
    TRAIN_ITERS=${23}
    LR_WARMUP_ITERS=${24}
    LR_DECAY_ITERS=$(( ${TRAIN_ITERS} - ${LR_WARMUP_ITERS}))
    PREFIX="finetune-mcore-llama3-1-${MODEL_SIZE}-lr-${LR}-minlr-${MIN_LR}-bs-${BATCH_SIZE}-gbs-${GLOBAL_BATCH_SIZE}-seqlen-${SEQ_LEN}"
    sft_option=" \
         --eod-mask-loss \
         --train-mode finetune"
else
    TRAIN_ITERS=$(( ${TRAIN_TOKENS} / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    LR_WARMUP_ITERS=$(( ${WARMUP_TOKENS}  / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    LR_DECAY_ITERS=$(( ${TRAIN_TOKENS} /  ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    PREFIX="pretrain-mcore-llama3-1-${MODEL_SIZE}-lr-${LR}-minlr-${MIN_LR}-bs-${BATCH_SIZE}-gbs-${GLOBAL_BATCH_SIZE}-seqlen-${SEQ_LEN}"
    sft_option=" \
        --train-mode pretrain"
fi

if [ ${MP_DATASET_TYPE} = "raw" ]; then
    dataset_option=" \
        --train-data-path ${DATASET_PATH} \
        --valid-data-path ${VALID_DATASET_PATH} \
        --dataloader-type cyclic \
        --dataset LLama-SFT-Raw"
else 
    dataset_option=" \
        --data-path ${DATASET_PATH} \
        --split 99,1,0 \
        --dataset LLama-Pretrain-Idxmap"
fi

if [ ${MP_SFT_PACKING} = true ]; then
    packing_options=" \
        --reset-position-ids \
        --no-create-attention-mask-in-dataloader
    "
else
    packing_options=""
fi


##### Prepare logdirs #######
NAME="${PREFIX}-pr-${PR}-tp-${TP}-pp-${PP}-cp-${CP}-ac-${AC}-do-${DO}-sp-${SP}-ti-${TRAIN_ITERS}-wi-${LR_WARMUP_ITERS}"
mkdir -p "${OUTPUT_BASEPATH}/tensorboard/"
mkdir -p "${OUTPUT_BASEPATH}/checkpoint/"
mkdir -p "${OUTPUT_BASEPATH}/log/"
current_time=$(date "+%Y.%m.%d-%H.%M.%S")
TENSORBOARD_DIR="${OUTPUT_BASEPATH}/tensorboard/${NAME}_${current_time}"
mkdir -p ${TENSORBOARD_DIR}
SAVED_PRETRAIN_CHECKPOINT_PATH="${OUTPUT_BASEPATH}/checkpoint/${NAME}"

mkdir -p ${SAVED_PRETRAIN_CHECKPOINT_PATH}
find -L ${PRETRAIN_CHECKPOINT_PATH} -maxdepth 1 -type f -name "*.json" -print0 | xargs -0 cp -t ${SAVED_PRETRAIN_CHECKPOINT_PATH}


megatron_options="  \
        --save ${SAVED_PRETRAIN_CHECKPOINT_PATH} \
        --lr ${LR} \
        --min-lr ${MIN_LR} \
        --lr-decay-style cosine \
        --weight-decay 0.1 \
        --adam-beta1 0.9 \
        --adam-beta2 0.95 \
        --clip-grad 1.0 \
        --init-method-std 0.008 \
        --attention-dropout 0.0 \
        --hidden-dropout 0.0 \
        --lr-decay-iters ${LR_DECAY_ITERS} \
        --lr-warmup-iters ${LR_WARMUP_ITERS} \
        --train-iters ${TRAIN_ITERS} \
        --micro-batch-size ${BATCH_SIZE} \
        --global-batch-size ${GLOBAL_BATCH_SIZE} \
        --num-layers ${NUM_LAYERS} \
        --hidden-size ${HIDDEN_SIZE} \
        --num-attention-heads ${NUM_ATTN_HEADS} \
        --ffn-hidden-size ${INTERMEDIATE_SIZE} \
        --seq-length ${SEQ_LEN} \
        --max-position-embeddings ${MAX_POSITION_EMBEDDINGS} \
        --max-padding-length ${PAD_LEN} \
        --log-interval 1 \
        --log-throughput \
        --eval-interval 10000 \
        --eval-iters 10 \
        --save-interval ${SAVE_INTERVAL} \
        --tensorboard-queue-size 1 \
        --tensorboard-dir ${TENSORBOARD_DIR} \
        --log-timers-to-tensorboard \
        --log-batch-size-to-tensorboard \
        --log-validation-ppl-to-tensorboard \
        --tensor-model-parallel-size ${TP} \
        --pipeline-model-parallel-size ${PP} \
        --context-parallel-size ${CP} \
        --no-load-optim \
        --no-load-rng \
        --num-workers 8 \
        --extra-vocab-size ${EXTRA_VOCAB_SIZE} \
        --patch-tokenizer-type LLama3Tokenizer \
        --swiglu \
        --normalization RMSNorm \
        --norm-epsilon 1e-05 \
        --use-rotary-position-embeddings \
        --position-embedding-type rope \
        --untie-embeddings-and-output-weights \
        --disable-bias-linear \
        --rotary-base 500000 \
        --no-save-optim \
        "

run_cmd="torchrun $DISTRIBUTED_ARGS pretrain_llama.py
 ${megatron_options} ${dataset_option} ${pr_options} ${load_options} ${te_options} ${activation_checkpoint_options} \
 ${do_options} ${sp_options} ${gqa_options} ${offload_option} ${comm_overlap_option} ${sft_option} ${vp_options} ${packing_options} 2>&1 |tee ${OUTPUT_DIR}/logs/training-log-$RANK.${DATETIME}.txt"

echo ${run_cmd}
eval ${run_cmd}
set +x
```







```bash
torchrun --nproc_per_node 8 --nnodes 4 --rdzv_id=333 --rdzv_backend=c10d --rdzv_endpoint llama3-fp8-tp1-pp2-cp2-ac-none-virtual4-2a5fa-master-0:26661 --tee 3 --log_dir /mnt/nfs129/zhongziban/output/llama3/llama3-fp8-TP1-PP2-CP2-AC-none-virtual4/logs/20241112T033727 pretrain_llama.py --save /mnt/nfs129/zhongziban/output/llama3/llama3-fp8-TP1-PP2-CP2-AC-none-virtual4/checkpoint/pretrain-mcore-llama3-1-8B-lr-1e-5-minlr-1e-6-bs-1-gbs-1024-seqlen-8192-pr-fp8-tp-1-pp-2-cp-2-ac-false-do-true-sp-true-ti-11920-wi-11 --lr 1e-5 --min-lr 1e-6 --lr-decay-style cosine --weight-decay 0.1 --adam-beta1 0.9 --adam-beta2 0.95 --clip-grad 1.0 --init-method-std 0.008 --attention-dropout 0.0 --hidden-dropout 0.0 --lr-decay-iters 11920 --lr-warmup-iters 11 --train-iters 11920 --micro-batch-size 1 --global-batch-size 1024 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --ffn-hidden-size 14336 --seq-length 8192 --max-position-embeddings 131072 --max-padding-length 8192 --log-interval 1 --log-throughput --eval-interval 10000 --eval-iters 10 --save-interval 100000 --tensorboard-queue-size 1 --tensorboard-dir /mnt/nfs129/zhongziban/output/llama3/llama3-fp8-TP1-PP2-CP2-AC-none-virtual4/tensorboard/pretrain-mcore-llama3-1-8B-lr-1e-5-minlr-1e-6-bs-1-gbs-1024-seqlen-8192-pr-fp8-tp-1-pp-2-cp-2-ac-false-do-true-sp-true-ti-11920-wi-11_2024.11.12-03.37.27 --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --context-parallel-size 2 --no-load-optim --no-load-rng --num-workers 8 --extra-vocab-size 256 --patch-tokenizer-type LLama3Tokenizer --swiglu --normalization RMSNorm --norm-epsilon 1e-05 --use-rotary-position-embeddings --position-embedding-type rope --untie-embeddings-and-output-weights --disable-bias-linear --rotary-base 500000 --no-save-optim --data-path /mnt/nfs129/zhongziban/dataset/llama3-dataset/wudao_llama3bpe_content_document --split 99,1,0 --dataset LLama-Pretrain-Idxmap --bf16 --fp8-format hybrid --fp8-amax-compute-algo max --fp8-amax-history-len 1024 --load /mnt/nfs129/zhongziban/model/Meta-Llama-3.1-8B-tokenizer/ --transformer-impl transformer_engine --use-distributed-optimizer --group-query-attention --num-query-groups 8 --overlap-grad-reduce --overlap-param-gather --train-mode pretrain --num-layers-per-virtual-pipeline-stage 4 2>&1 |tee /mnt/nfs129/zhongziban/output/llama3/llama3-fp8-TP1-PP2-CP2-AC-none-virtual4/logs/training-log-3.20241112T033727.txt
```



## Qwen2.5-14B

### 启动脚本

#### Pai-Megatron-Patch脚本

```bash
#!/bin/bash
set -e
ENV=$1
CURRENT_DIR="$( cd "$( dirname "$0" )" && pwd )"
MEGATRON_PATH=$( dirname $( dirname ${CURRENT_DIR}))
export PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATH}/PAI-Megatron-LM-240718:$PYTHONPATH
export CUDA_DEVICE_MAX_CONNECTIONS=1
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Here are some configs controled by env
if [ -z ${MP_DATASET_TYPE} ];then
    MP_DATASET_TYPE="idxmap"
fi

if [ -z ${MP_AC_LAYERS} ];then
    MP_AC_LAYERS=1
fi

if [ $ENV = dsw ]; then
    export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
    MASTER_ADDR=localhost
    MASTER_PORT=$(shuf -n 1 -i 10000-65535)
    NNODES=1
    NODE_RANK=0
    GPUS_PER_NODE=8
elif [ $ENV = dlc ]; then
    NNODES=${WORLD_SIZE}
    NODE_RANK=${RANK}
    GPUS_PER_NODE=${KUBERNETES_CONTAINER_RESOURCE_GPU}
fi

if [ -z ${MP_VP} ]; then
    vp_options=""
else
    vp_options=" \
        --num-layers-per-virtual-pipeline-stage ${MP_VP}"
fi

if [ -z ${MP_SFT_PACKING} ]; then
    MP_SFT_PACKING=false
fi


DISTRIBUTED_ARGS="--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT"

### BASE CONFIG ###
MODEL_SIZE=$2
BATCH_SIZE=$3
GLOBAL_BATCH_SIZE=$4
LR=$5
MIN_LR=$6
SEQ_LEN=$7
PAD_LEN=$8
PR=$9
### BASE CONFIG ###

### PARALLEL / BOOL OPTION ###
TP=${10}
PP=${11}
CP=${12}
SP=${13}
DO=${14}
FL=${15}
SFT=${16}
### PARALLEL / BOOL OPTION ###

### OTHERS ###
AC=${17}
OPTIMIZER_OFFLOAD=${18}
SAVE_INTERVAL=${19}
DATASET_PATH=${20}
VALID_DATASET_PATH=${21}
PRETRAIN_CHECKPOINT_PATH=${22}

# the following two values will not be used when SFT is true
TRAIN_TOKENS=${23}
WARMUP_TOKENS=${24}
###############################

OUTPUT_BASEPATH=${25}
### OTHERS ###

if [ $FL = true ]; then
    export NVTE_FLASH_ATTN=1 NVTE_FUSED_ATTN=0
elif [ $FL = false ]; then
    export NVTE_FLASH_ATTN=0 NVTE_FUSED_ATTN=1
fi

if [ $MODEL_SIZE = 0.5B ]; then

NUM_LAYERS=24
HIDDEN_SIZE=896
NUM_ATTN_HEADS=14
INTERMEDIATE_SIZE=4864
NUM_KEY_VALUE_HEADS=2
MAX_POSITION_EMBEDDINGS=32768
EXTRA_VOCAB_SIZE=293
RMS_NORM_EPS=1e-6
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"


tie_option=""

elif [ $MODEL_SIZE = 1.5B ]; then

NUM_LAYERS=28
HIDDEN_SIZE=1536
NUM_ATTN_HEADS=12
INTERMEDIATE_SIZE=8960
NUM_KEY_VALUE_HEADS=2
MAX_POSITION_EMBEDDINGS=32768
EXTRA_VOCAB_SIZE=293
RMS_NORM_EPS=1e-6
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

tie_option=""

elif [ $MODEL_SIZE = 3B ]; then

NUM_LAYERS=36
HIDDEN_SIZE=2048
NUM_ATTN_HEADS=16
INTERMEDIATE_SIZE=11008
NUM_KEY_VALUE_HEADS=2
MAX_POSITION_EMBEDDINGS=32768
EXTRA_VOCAB_SIZE=293
RMS_NORM_EPS=1e-6
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

tie_option=""

elif [ $MODEL_SIZE = 7B ]; then

NUM_LAYERS=28
HIDDEN_SIZE=3584
NUM_ATTN_HEADS=28
INTERMEDIATE_SIZE=18944
NUM_KEY_VALUE_HEADS=4
MAX_POSITION_EMBEDDINGS=131072
EXTRA_VOCAB_SIZE=421
RMS_NORM_EPS=1e-6
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

tie_option=" \
        --untie-embeddings-and-output-weights \
        "

elif [ $MODEL_SIZE = 14B ]; then

NUM_LAYERS=48
HIDDEN_SIZE=5120
NUM_ATTN_HEADS=40
INTERMEDIATE_SIZE=13824
NUM_KEY_VALUE_HEADS=8
MAX_POSITION_EMBEDDINGS=131072
EXTRA_VOCAB_SIZE=421
RMS_NORM_EPS=1e-5
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

tie_option=" \
        --untie-embeddings-and-output-weights \
        "
elif [ $MODEL_SIZE = 32B ]; then

NUM_LAYERS=64
HIDDEN_SIZE=5120
NUM_ATTN_HEADS=40
INTERMEDIATE_SIZE=27648
NUM_KEY_VALUE_HEADS=8
MAX_POSITION_EMBEDDINGS=131072
EXTRA_VOCAB_SIZE=421
RMS_NORM_EPS=1e-5
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

tie_option=" \
        --untie-embeddings-and-output-weights \
        "
elif [ $MODEL_SIZE = 72B ]; then

NUM_LAYERS=80
HIDDEN_SIZE=8192
NUM_ATTN_HEADS=64
INTERMEDIATE_SIZE=29568
NUM_KEY_VALUE_HEADS=8
MAX_POSITION_EMBEDDINGS=131072
EXTRA_VOCAB_SIZE=421
RMS_NORM_EPS=1e-5
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

tie_option=" \
        --untie-embeddings-and-output-weights \
        "

fi

TP_COMM_OVERLAP=$(( ($TP > 1) ? 1 : 0 ))
comm_overlap_option="\
    --overlap-grad-reduce \
    --overlap-param-gather"
 

if [ $TP_COMM_OVERLAP -eq 1 ]; then
    comm_overlap_option="\
        --tp-comm-overlap \
        --overlap-grad-reduce \
        --overlap-param-gather"
fi

if [ $AC = full ]; then
    _check=$(( ($NUM_LAYERS / $PP) % ${MP_AC_LAYERS} ))
    if [ $_check != 0 ]; then
        echo "the num layers per pp rank must be a multiple of the recompute layers."
        exit -1
    fi
    activation_checkpoint_options=" \
                    --recompute-method uniform \
            --recompute-num-layers ${MP_AC_LAYERS} \
                    --recompute-granularity full"
elif [ $AC = sel ]; then
    activation_checkpoint_options=" \
        --recompute-activations"
elif [ $AC = none ]; then
    activation_checkpoint_options=" \
    "
elif [ $AC = offload ]; then
    activation_checkpoint_options=" \
                    --cpu-offloading \
                    --cpu-offloading-num-layers ${MP_AC_LAYERS}"
    if [ $TP_COMM_OVERLAP -eq 1 ]; then
        echo "Disable --overlap-grad-reduce and --overlap-param-gather when cpu offloading is on..."
        comm_overlap_option="\
            --tp-comm-overlap"
    else
        echo "Disable --overlap-grad-reduce and --overlap-param-gather when cpu offloading is on..."
        comm_overlap_option=""
    fi
fi

if [ $PR = fp16 ]; then
    pr_options=" \
                    --fp16 \
            --apply-query-key-layer-scaling"
    export NVTE_APPLY_QK_LAYER_SCALING=1
elif [ $PR = bf16 ]; then
    pr_options=" \
        --bf16"
elif [ $PR = fp8 ]; then
    pr_options=" \
        --bf16 \
        --fp8-format hybrid \
        --fp8-amax-compute-algo max \
        --fp8-amax-history-len 1024"
fi

if [ $OPTIMIZER_OFFLOAD != false ] && [ $DO = false ]; then
    echo "Offload optimizer is valid only if \$DO=true"
    DO=true
fi

if [ $DO = true ]; then
    do_options=" \
                    --use-distributed-optimizer"

elif [ $DO = false ]; then
    do_options=" \
                    "
fi

te_options=" \
        --transformer-impl transformer_engine"

if [ $SP = true ] && [ $TP -gt 1 ]; then
    sp_options=" \
                    --sequence-parallel"

elif [ $SP = false ]; then
    sp_options=" \
                    "
fi

if [ $PRETRAIN_CHECKPOINT_PATH != none ]; then
    load_options=" \
            --load $PRETRAIN_CHECKPOINT_PATH"
fi

if [ $OPTIMIZER_OFFLOAD = 'static' ]; then
    offload_option=" \
        --optimizer hybridadam \
        --optimizer-offload-policy static \
        --optimizer-offload-fraction 1.0"
elif [ $OPTIMIZER_OFFLOAD = 'auto' ]; then
    offload_option=" \
        --optimizer hybridadam \
        --optimizer-offload-policy auto"
else
    offload_option=""
fi

if [ $SFT = true ]; then
    TRAIN_ITERS=${23}
    LR_WARMUP_ITERS=${24}
    LR_DECAY_ITERS=$(( ${TRAIN_ITERS} - ${LR_WARMUP_ITERS}))
    PREFIX="finetune-mcore-qwen2.5-${MODEL_SIZE}-lr-${LR}-minlr-${MIN_LR}-bs-${BATCH_SIZE}-gbs-${GLOBAL_BATCH_SIZE}-seqlen-${SEQ_LEN}"
    sft_option=" \
         --eod-mask-loss \
         --train-mode finetune"
else
    TRAIN_ITERS=$(( ${TRAIN_TOKENS} / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    LR_WARMUP_ITERS=$(( ${WARMUP_TOKENS}  / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    LR_DECAY_ITERS=$(( ${TRAIN_TOKENS} /  ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    PREFIX="pretrain-mcore-qwen2.5-${MODEL_SIZE}-lr-${LR}-minlr-${MIN_LR}-bs-${BATCH_SIZE}-gbs-${GLOBAL_BATCH_SIZE}-seqlen-${SEQ_LEN}"
    sft_option=" \
        --train-mode pretrain"
fi

if [ ${MP_DATASET_TYPE} = "raw" ]; then
    dataset_option=" \
        --train-data-path ${DATASET_PATH} \
        --valid-data-path ${VALID_DATASET_PATH} \
        --dataloader-type cyclic \
        --dataset LLama-SFT-Raw"
else 
    dataset_option=" \
        --data-path ${DATASET_PATH} \
        --split 99,1,0 \
        --dataset LLama-Pretrain-Idxmap"
fi

if [ ${MP_SFT_PACKING} = true ]; then
    packing_options=" \
        --reset-position-ids \
        --no-create-attention-mask-in-dataloader
    "
else
    packing_options=""
fi

##### Prepare logdirs #######
NAME="${PREFIX}-pr-${PR}-tp-${TP}-pp-${PP}-cp-${CP}-ac-${AC}-do-${DO}-sp-${SP}-ti-${TRAIN_ITERS}-wi-${LR_WARMUP_ITERS}"
mkdir -p "${OUTPUT_BASEPATH}/tensorboard/"
mkdir -p "${OUTPUT_BASEPATH}/checkpoint/"
mkdir -p "${OUTPUT_BASEPATH}/log/"
current_time=$(date "+%Y.%m.%d-%H.%M.%S")
TENSORBOARD_DIR="${OUTPUT_BASEPATH}/tensorboard/${NAME}_${current_time}"
mkdir -p ${TENSORBOARD_DIR}
SAVED_PRETRAIN_CHECKPOINT_PATH="${OUTPUT_BASEPATH}/checkpoint/${NAME}"

mkdir -p ${SAVED_PRETRAIN_CHECKPOINT_PATH}
find -L ${PRETRAIN_CHECKPOINT_PATH} -maxdepth 1 -type f -name "*.json" -print0 | xargs -0 cp -t ${SAVED_PRETRAIN_CHECKPOINT_PATH}
find -L ${PRETRAIN_CHECKPOINT_PATH} -maxdepth 1 -type f -name "merges.txt" -print0 | xargs -0 cp -t ${SAVED_PRETRAIN_CHECKPOINT_PATH}

megatron_options="  \
        --save ${SAVED_PRETRAIN_CHECKPOINT_PATH} \
        --lr ${LR} \
        --min-lr ${MIN_LR} \
        --lr-decay-style cosine \
        --weight-decay 0.1 \
        --adam-beta1 0.9 \
        --adam-beta2 0.95 \
        --clip-grad 1.0 \
        --init-method-std 0.008 \
        --attention-dropout 0.0 \
        --hidden-dropout 0.0 \
        --lr-decay-iters ${LR_DECAY_ITERS} \
        --lr-warmup-iters ${LR_WARMUP_ITERS} \
        --train-iters ${TRAIN_ITERS} \
        --micro-batch-size ${BATCH_SIZE} \
        --global-batch-size ${GLOBAL_BATCH_SIZE} \
        --num-layers ${NUM_LAYERS} \
        --hidden-size ${HIDDEN_SIZE} \
        --num-attention-heads ${NUM_ATTN_HEADS} \
        --ffn-hidden-size ${INTERMEDIATE_SIZE} \
        --seq-length ${SEQ_LEN} \
        --max-position-embeddings ${MAX_POSITION_EMBEDDINGS} \
        --max-padding-length ${PAD_LEN} \
        --log-interval 1 \
        --log-throughput \
        --eval-interval 10000 \
        --eval-iters 10 \
        --save-interval ${SAVE_INTERVAL} \
        --tensorboard-queue-size 1 \
        --tensorboard-dir ${TENSORBOARD_DIR} \
        --log-timers-to-tensorboard \
        --log-batch-size-to-tensorboard \
        --log-validation-ppl-to-tensorboard \
        --tensor-model-parallel-size ${TP} \
        --pipeline-model-parallel-size ${PP} \
        --context-parallel-size ${CP} \
        --no-load-optim \
        --no-load-rng \
        --num-workers 8 \
        --extra-vocab-size ${EXTRA_VOCAB_SIZE} \
        --patch-tokenizer-type Qwen2Tokenizer \
        --swiglu \
        --normalization RMSNorm \
        --norm-epsilon ${RMS_NORM_EPS} \
        --use-rotary-position-embeddings \
        --position-embedding-type rope \
        --disable-bias-linear \
        --add-qkv-bias \
        --rotary-percent 1.0 \
        --rotary-base 1000000 \
        --rotary-seq-len-interpolation-factor 1 \
        --no-save-optim \
        "

run_cmd="torchrun $DISTRIBUTED_ARGS ../qwen2/pretrain_qwen.py
 ${megatron_options} ${dataset_option} ${pr_options} ${load_options} ${te_options} ${activation_checkpoint_options} \
 ${do_options} ${sp_options} ${gqa_options} ${offload_option} ${comm_overlap_option} ${sft_option}  ${tie_option} ${vp_options} ${packing_options}"

filename="torchrun/torchrun_$(date +"%Y-%m-%d_%H-%M-%S").log"
echo ${run_cmd} >"$filename"
eval ${run_cmd}
set +x
```

#### 种子班启动脚本

&emsp;主要是要修改数据集，创建目录等

> 注: 在启动之前包含cd到Pai-Megatron-Patch的路径。
>
> 注：参数PRETRAIN_CHECKPOINT_PATH=${22}  # 预训练模型路径在开头指定了。
>
> 注： 在种子班进行训练，需要使用的环境变量包括 ${WORLD_SIZE}、${TQ_GPU_NUM}，同时参数DISTRIBUTED_ARGS需要进行更新。

```bash
#!/bin/bash
set -e

OUTPUT_BASEPATH="$OUTPUT_DIR"
DATETIME="$(date +'%Y%m%dT%H%M%S')"
# 使用huggingface模型，则会随机进行初始化
PRETRAIN_CHECKPOINT_PATH="$(pwd)"/model/Qwen2.5-14B-tokenizer/

cd Pai-Megatron-Patch/examples/qwen2_5
ENV=dlc
CURRENT_DIR="$( cd "$( dirname "$0" )" && pwd )"
MEGATRON_PATH=$( dirname $( dirname ${CURRENT_DIR}))

LOG_DIR="${OUTPUT_BASEPATH}"

export PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATH}/PAI-Megatron-LM-240718:$PYTHONPATH
export CUDA_DEVICE_MAX_CONNECTIONS=1
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Here are some configs controled by env
if [ -z ${MP_DATASET_TYPE} ];then
    MP_DATASET_TYPE="idxmap"
fi

if [ -z ${MP_AC_LAYERS} ];then
    MP_AC_LAYERS=1
fi

if [ $ENV = dsw ]; then
    export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
    MASTER_ADDR=localhost
    MASTER_PORT=$(shuf -n 1 -i 10000-65535)
    NNODES=1
    NODE_RANK=0
    GPUS_PER_NODE=8
elif [ $ENV = dlc ]; then
    NNODES=${WORLD_SIZE}
    GPUS_PER_NODE=${TQ_GPU_NUM}
fi


if [ -z ${MP_SFT_PACKING} ]; then
    MP_SFT_PACKING=false
fi


DISTRIBUTED_ARGS="--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES \
  --rdzv_id=333 --rdzv_backend=c10d --rdzv_endpoint $MASTER_ADDR:$MASTER_PORT \
  --tee 3 --log_dir ${LOG_DIR}/logs/${DATETIME}"

### BASE CONFIG ###
MODEL_SIZE=14B
BATCH_SIZE=1
GLOBAL_BATCH_SIZE=1024
LR=1e-5
MIN_LR=1e-6
SEQ_LEN=8192
PAD_LEN=8192
PR=fp8
### BASE CONFIG ###

### PARALLEL / BOOL OPTION ###
TP=1
PP=4
CP=1
SP=true
DO=true
FL=true
SFT=false
### PARALLEL / BOOL OPTION ###

### OTHERS ###
AC=false
OPTIMIZER_OFFLOAD=false
SAVE_INTERVAL=10000
DATASET_PATH=/mnt/nfs129/zhongziban/dataset/qwen-dataset/wudao_qwenbpe_content_document
VALID_DATASET_PATH=/mnt/nfs129/zhongziban/dataset/qwen-dataset/wudao_qwenbpe_content_document


# the following two values will not be used when SFT is true
TRAIN_TOKENS=100000000000
WARMUP_TOKENS=100000000

###############################
### OTHERS ###

if [ $FL = true ]; then
    export NVTE_FLASH_ATTN=1 NVTE_FUSED_ATTN=0
elif [ $FL = false ]; then
    export NVTE_FLASH_ATTN=0 NVTE_FUSED_ATTN=1
fi

if [ $MODEL_SIZE = 0.5B ]; then

NUM_LAYERS=24
HIDDEN_SIZE=896
NUM_ATTN_HEADS=14
INTERMEDIATE_SIZE=4864
NUM_KEY_VALUE_HEADS=2
MAX_POSITION_EMBEDDINGS=32768
EXTRA_VOCAB_SIZE=293
RMS_NORM_EPS=1e-6
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"


tie_option=""

elif [ $MODEL_SIZE = 1.5B ]; then

NUM_LAYERS=28
HIDDEN_SIZE=1536
NUM_ATTN_HEADS=12
INTERMEDIATE_SIZE=8960
NUM_KEY_VALUE_HEADS=2
MAX_POSITION_EMBEDDINGS=32768
EXTRA_VOCAB_SIZE=293
RMS_NORM_EPS=1e-6
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

tie_option=""

elif [ $MODEL_SIZE = 3B ]; then

NUM_LAYERS=36
HIDDEN_SIZE=2048
NUM_ATTN_HEADS=16
INTERMEDIATE_SIZE=11008
NUM_KEY_VALUE_HEADS=2
MAX_POSITION_EMBEDDINGS=32768
EXTRA_VOCAB_SIZE=293
RMS_NORM_EPS=1e-6
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

tie_option=""

elif [ $MODEL_SIZE = 7B ]; then

NUM_LAYERS=28
HIDDEN_SIZE=3584
NUM_ATTN_HEADS=28
INTERMEDIATE_SIZE=18944
NUM_KEY_VALUE_HEADS=4
MAX_POSITION_EMBEDDINGS=131072
EXTRA_VOCAB_SIZE=421
RMS_NORM_EPS=1e-6
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

tie_option=" \
        --untie-embeddings-and-output-weights \
        "

elif [ $MODEL_SIZE = 14B ]; then

NUM_LAYERS=48
HIDDEN_SIZE=5120
NUM_ATTN_HEADS=40
INTERMEDIATE_SIZE=13824
NUM_KEY_VALUE_HEADS=8
MAX_POSITION_EMBEDDINGS=131072
EXTRA_VOCAB_SIZE=421
RMS_NORM_EPS=1e-5
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

tie_option=" \
        --untie-embeddings-and-output-weights \
        "
elif [ $MODEL_SIZE = 32B ]; then

NUM_LAYERS=64
HIDDEN_SIZE=5120
NUM_ATTN_HEADS=40
INTERMEDIATE_SIZE=27648
NUM_KEY_VALUE_HEADS=8
MAX_POSITION_EMBEDDINGS=131072
EXTRA_VOCAB_SIZE=421
RMS_NORM_EPS=1e-5
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

tie_option=" \
        --untie-embeddings-and-output-weights \
        "
elif [ $MODEL_SIZE = 72B ]; then

NUM_LAYERS=80
HIDDEN_SIZE=8192
NUM_ATTN_HEADS=64
INTERMEDIATE_SIZE=29568
NUM_KEY_VALUE_HEADS=8
MAX_POSITION_EMBEDDINGS=131072
EXTRA_VOCAB_SIZE=421
RMS_NORM_EPS=1e-5
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

tie_option=" \
        --untie-embeddings-and-output-weights \
        "

fi


MP_VP=$(( $NUM_LAYERS / $PP / 4 ))

if [ -z ${MP_VP} ]; then
    vp_options=""
else
    vp_options=" \
        --num-layers-per-virtual-pipeline-stage ${MP_VP}"
fi


TP_COMM_OVERLAP=$(( ($TP > 1) ? 1 : 0 ))
comm_overlap_option="\
    --overlap-grad-reduce \
    --overlap-param-gather"
 

if [ $TP_COMM_OVERLAP -eq 1 ]; then
    comm_overlap_option="\
        --tp-comm-overlap \
        --overlap-grad-reduce \
        --overlap-param-gather"
fi

if [ $AC = full ]; then
    _check=$(( ($NUM_LAYERS / $PP) % ${MP_AC_LAYERS} ))
    if [ $_check != 0 ]; then
        echo "the num layers per pp rank must be a multiple of the recompute layers."
        exit -1
    fi
    activation_checkpoint_options=" \
                    --recompute-method uniform \
            --recompute-num-layers ${MP_AC_LAYERS} \
                    --recompute-granularity full"
elif [ $AC = sel ]; then
    activation_checkpoint_options=" \
        --recompute-activations"
elif [ $AC = none ]; then
    activation_checkpoint_options=" \
    "
elif [ $AC = offload ]; then
    activation_checkpoint_options=" \
                    --cpu-offloading \
                    --cpu-offloading-num-layers ${MP_AC_LAYERS}"
    if [ $TP_COMM_OVERLAP -eq 1 ]; then
        echo "Disable --overlap-grad-reduce and --overlap-param-gather when cpu offloading is on..."
        comm_overlap_option="\
            --tp-comm-overlap"
    else
        echo "Disable --overlap-grad-reduce and --overlap-param-gather when cpu offloading is on..."
        comm_overlap_option=""
    fi
fi

if [ $PR = fp16 ]; then
    pr_options=" \
                    --fp16 \
            --apply-query-key-layer-scaling"
    export NVTE_APPLY_QK_LAYER_SCALING=1
elif [ $PR = bf16 ]; then
    pr_options=" \
        --bf16"
elif [ $PR = fp8 ]; then
    pr_options=" \
        --bf16 \
        --fp8-format hybrid \
        --fp8-amax-compute-algo max \
        --fp8-amax-history-len 1024"
fi

if [ $OPTIMIZER_OFFLOAD != false ] && [ $DO = false ]; then
    echo "Offload optimizer is valid only if \$DO=true"
    DO=true
fi

if [ $DO = true ]; then
    do_options=" \
                    --use-distributed-optimizer"

elif [ $DO = false ]; then
    do_options=" \
                    "
fi

te_options=" \
        --transformer-impl transformer_engine"

if [ $SP = true ] && [ $TP -gt 1 ]; then
    sp_options=" \
                    --sequence-parallel"

elif [ $SP = false ]; then
    sp_options=" \
                    "
fi

if [ $PRETRAIN_CHECKPOINT_PATH != none ]; then
    load_options=" \
            --load $PRETRAIN_CHECKPOINT_PATH"
fi

if [ $OPTIMIZER_OFFLOAD = 'static' ]; then
    offload_option=" \
        --optimizer hybridadam \
        --optimizer-offload-policy static \
        --optimizer-offload-fraction 1.0"
elif [ $OPTIMIZER_OFFLOAD = 'auto' ]; then
    offload_option=" \
        --optimizer hybridadam \
        --optimizer-offload-policy auto"
else
    offload_option=""
fi

if [ $SFT = true ]; then
    TRAIN_ITERS=${23}
    LR_WARMUP_ITERS=${24}
    LR_DECAY_ITERS=$(( ${TRAIN_ITERS} - ${LR_WARMUP_ITERS}))
    PREFIX="finetune-mcore-qwen2.5-${MODEL_SIZE}-lr-${LR}-minlr-${MIN_LR}-bs-${BATCH_SIZE}-gbs-${GLOBAL_BATCH_SIZE}-seqlen-${SEQ_LEN}"
    sft_option=" \
         --eod-mask-loss \
         --train-mode finetune"
else
    TRAIN_ITERS=$(( ${TRAIN_TOKENS} / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    LR_WARMUP_ITERS=$(( ${WARMUP_TOKENS}  / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    LR_DECAY_ITERS=$(( ${TRAIN_TOKENS} /  ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    PREFIX="pretrain-mcore-qwen2.5-${MODEL_SIZE}-lr-${LR}-minlr-${MIN_LR}-bs-${BATCH_SIZE}-gbs-${GLOBAL_BATCH_SIZE}-seqlen-${SEQ_LEN}"
    sft_option=" \
        --train-mode pretrain"
fi

if [ ${MP_DATASET_TYPE} = "raw" ]; then
    dataset_option=" \
        --train-data-path ${DATASET_PATH} \
        --valid-data-path ${VALID_DATASET_PATH} \
        --dataloader-type cyclic \
        --dataset LLama-SFT-Raw"
else 
    dataset_option=" \
        --data-path ${DATASET_PATH} \
        --split 99,1,0 \
        --dataset LLama-Pretrain-Idxmap"
fi

if [ ${MP_SFT_PACKING} = true ]; then
    packing_options=" \
        --reset-position-ids \
        --no-create-attention-mask-in-dataloader
    "
else
    packing_options=""
fi

##### Prepare logdirs #######
NAME="${PREFIX}-pr-${PR}-tp-${TP}-pp-${PP}-cp-${CP}-ac-${AC}-do-${DO}-sp-${SP}-ti-${TRAIN_ITERS}-wi-${LR_WARMUP_ITERS}"
mkdir -p "${OUTPUT_BASEPATH}/tensorboard/"
mkdir -p "${OUTPUT_BASEPATH}/checkpoint/"
mkdir -p "${OUTPUT_BASEPATH}/log/"
current_time=$(date "+%Y.%m.%d-%H.%M.%S")
TENSORBOARD_DIR="${OUTPUT_BASEPATH}/tensorboard/${NAME}_${current_time}"
mkdir -p ${TENSORBOARD_DIR}
SAVED_PRETRAIN_CHECKPOINT_PATH="${OUTPUT_BASEPATH}/checkpoint/${NAME}"

mkdir -p ${SAVED_PRETRAIN_CHECKPOINT_PATH}
find -L ${PRETRAIN_CHECKPOINT_PATH} -maxdepth 1 -type f -name "*.json" -print0 | xargs -0 cp -t ${SAVED_PRETRAIN_CHECKPOINT_PATH}
find -L ${PRETRAIN_CHECKPOINT_PATH} -maxdepth 1 -type f -name "merges.txt" -print0 | xargs -0 cp -t ${SAVED_PRETRAIN_CHECKPOINT_PATH}

megatron_options="  \
        --save ${SAVED_PRETRAIN_CHECKPOINT_PATH} \
        --lr ${LR} \
        --min-lr ${MIN_LR} \
        --lr-decay-style cosine \
        --weight-decay 0.1 \
        --adam-beta1 0.9 \
        --adam-beta2 0.95 \
        --clip-grad 1.0 \
        --init-method-std 0.008 \
        --attention-dropout 0.0 \
        --hidden-dropout 0.0 \
        --lr-decay-iters ${LR_DECAY_ITERS} \
        --lr-warmup-iters ${LR_WARMUP_ITERS} \
        --train-iters ${TRAIN_ITERS} \
        --micro-batch-size ${BATCH_SIZE} \
        --global-batch-size ${GLOBAL_BATCH_SIZE} \
        --num-layers ${NUM_LAYERS} \
        --hidden-size ${HIDDEN_SIZE} \
        --num-attention-heads ${NUM_ATTN_HEADS} \
        --ffn-hidden-size ${INTERMEDIATE_SIZE} \
        --seq-length ${SEQ_LEN} \
        --max-position-embeddings ${MAX_POSITION_EMBEDDINGS} \
        --max-padding-length ${PAD_LEN} \
        --log-interval 1 \
        --log-throughput \
        --eval-interval 10000 \
        --eval-iters 10 \
        --save-interval ${SAVE_INTERVAL} \
        --tensorboard-queue-size 1 \
        --tensorboard-dir ${TENSORBOARD_DIR} \
        --log-timers-to-tensorboard \
        --log-batch-size-to-tensorboard \
        --log-validation-ppl-to-tensorboard \
        --tensor-model-parallel-size ${TP} \
        --pipeline-model-parallel-size ${PP} \
        --context-parallel-size ${CP} \
        --no-load-optim \
        --no-load-rng \
        --num-workers 8 \
        --extra-vocab-size ${EXTRA_VOCAB_SIZE} \
        --patch-tokenizer-type Qwen2Tokenizer \
        --swiglu \
        --normalization RMSNorm \
        --norm-epsilon ${RMS_NORM_EPS} \
        --use-rotary-position-embeddings \
        --position-embedding-type rope \
        --disable-bias-linear \
        --add-qkv-bias \
        --rotary-percent 1.0 \
        --rotary-base 1000000 \
        --rotary-seq-len-interpolation-factor 1 \
        --no-save-optim \
        "

run_cmd="torchrun $DISTRIBUTED_ARGS ../qwen2/pretrain_qwen.py
 ${megatron_options} ${dataset_option} ${pr_options} ${load_options} ${te_options} ${activation_checkpoint_options} \
 ${do_options} ${sp_options} ${gqa_options} ${offload_option} ${comm_overlap_option} ${sft_option}  ${tie_option} ${vp_options} ${packing_options} 2>&1 |tee ${OUTPUT_DIR}/logs/training-log-$RANK.${DATETIME}.txt"

filename="${OUTPUT_BASEPATH}/log/torchrun_$(date +"%Y-%m-%d_%H-%M-%S").log"
echo ${run_cmd} >"$filename"
eval ${run_cmd}
set +x
```



### 采样数据-Qwen2.5-14B-4机

> 一共48层,PP=4

```bash
MP_VP=$(( $NUM_LAYERS / $PP / 4 ))

if [ -z ${MP_VP} ]; then
    vp_options=""
else
    vp_options=" \
        --num-layers-per-virtual-pipeline-stage ${MP_VP}"
fi

# 取经验最佳为3
```



| precision | TP   | PP   | CP   | AC      | num-layers-<br />per-virtual-<br />pipeline-stage | TFLOPS | elapsed time | memory                                                       |
| --------- | ---- | ---- | ---- | ------- | ------------------------------------------------- | ------ | ------------ | ------------------------------------------------------------ |
| bf8       | 1    | 4    | 1    | none    | 3                                                 |        |              | OOM                                                          |
| bf8       | 1    | 4    | 1    | sel     | 3                                                 |        |              | OOM                                                          |
| bf8       | 1    | 4    | 1    | full    | 3                                                 | 518.3  | 54675.9      | 52252MiB                                                     |
| bf8       | 1    | 4    | 1    | offload | 3                                                 |        |              | ValueError: Currently there is no support for Pipeline parallelism with CPU offloading |
| bf8       | 1    | 8    | 1    | none    | 2                                                 |        |              |                                                              |
| bf8       | 1    | 8    | 1    | sel     | 2                                                 |        |              |                                                              |
| bf8       | 1    | 8    | 1    | full    | 2                                                 |        |              |                                                              |
| bf8       | 2    | 8    | 1    | none    | 2                                                 | 452.1  | 62687.8      | 51998MiB                                                     |
| bf8       | 2    | 2    | 1    | none    | 3                                                 | 629.6  | 45054.7      | 66930MiB                                                     |
| bf8       | 2    | 2    | 1    | none    | 5                                                 |        |              | AssertionError: number of layers per pipeline stage must be divisible number of layers per virtual pipeline stage |
| bf8       | 2    | 2    | 1    | none    | 6                                                 | 632.7  | 44789.2      | 67404MiB                                                     |
| bf16      | 2    | 2    | 1    | none    | 6                                                 | 479.1  | 59146.6      | 64814MiB                                                     |
| bf8       | 2    | 2    | 1    | sel     | 6                                                 | 586.5  | 48319.5      | 67870MiB                                                     |
| bf8       | 2    | 2    | 1    | full    | 6                                                 | 629.8  | 44998.8      | 67424MiB                                                     |
| bf8       | 2    | 2    | 2    | none    | 6                                                 | 574.0  | 49368.7      | 53728MiB                                                     |
|           |      |      |      |         |                                                   |        |              |                                                              |
| bf8       | 2    | 4    | 1    | none    | 3                                                 | 558.0  | 50784.5      | 57212MiB                                                     |
| bf8       | 2    | 4    | 1    | sel     | 3                                                 |        |              |                                                              |
| bf8       | 2    | 4    | 1    | full    | 3                                                 |        |              |                                                              |
|           |      |      |      |         |                                                   |        |              |                                                              |
| bf8       | 4    | 2    | 1    | none    | 3                                                 | 570.3  | 49694.3      | 38694MiB                                                     |
| bf8       | 4    | 2    | 1    | sel     | 3                                                 |        |              |                                                              |
| bf8       | 4    | 2    | 1    | full    | 3                                                 |        |              |                                                              |
| bf8       | 8    | 1    | 1    | none    | 3                                                 |        |              | OOM                                                          |
| bf8       | 8    | 1    | 1    | sel     | 3                                                 |        |              |                                                              |
| bf8       | 8    | 1    | 1    | full    | 3                                                 |        |              |                                                              |
| bf8       | 8    | 1    | 1    | offload | 3                                                 |        |              |                                                              |
|           |      |      |      |         |                                                   |        |              |                                                              |
|           |      |      |      |         |                                                   |        |              |                                                              |



## Llama3.1-70B-8机

### 70b-fp8-tp1-pp4-cp1-ac-none-vitual5

```bash
#!/bin/bash
# please cd to the dir containing 
set -e
OUTPUT_BASEPATH="$OUTPUT_DIR"
DATETIME="$(date +'%Y%m%dT%H%M%S')"
# 使用huggingface模型，则会随机进行初始化
PRETRAIN_CHECKPOINT_PATH="$(pwd)"/model/Meta-Llama-3.1-8B-tokenizer/

cd Pai-Megatron-Patch/examples/llama3_1
ENV=dlc
CURRENT_DIR="$( cd "$( dirname "$0" )" && pwd )"
MEGATRON_PATH=$( dirname $( dirname ${CURRENT_DIR}))

LOG_DIR="${OUTPUT_BASEPATH}"
export PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATH}/PAI-Megatron-LM-240718:$PYTHONPATH
export CUDA_DEVICE_MAX_CONNECTIONS=1
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
echo "CURRENT_DIR: ${CURRENT_DIR}"
echo "MEGATRON_PATH: ${MEGATRON_PATH}"
# Here are some configs controled by env
if [ -z ${MP_DATASET_TYPE} ];then
    MP_DATASET_TYPE="idxmap"
fi

if [ -z ${MP_AC_LAYERS} ];then
    MP_AC_LAYERS=1
fi

if [ $ENV = dsw ]; then
    export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
    MASTER_ADDR=localhost
    MASTER_PORT=$(shuf -n 1 -i 10000-65535)
    NNODES=1
    NODE_RANK=0
    GPUS_PER_NODE=8
elif [ $ENV = dlc ]; then
    NNODES=${WORLD_SIZE}
    GPUS_PER_NODE=${TQ_GPU_NUM}
fi



if [ -z ${MP_SFT_PACKING} ]; then
    MP_SFT_PACKING=false
fi


DISTRIBUTED_ARGS="--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES \
  --rdzv_id=333 --rdzv_backend=c10d --rdzv_endpoint $MASTER_ADDR:$MASTER_PORT \
  --tee 3 --log_dir ${LOG_DIR}/logs/${DATETIME}"

EXTRA_VOCAB_SIZE=256

### BASE CONFIG ###
MODEL_SIZE=70B
BATCH_SIZE=1
GLOBAL_BATCH_SIZE=1024
LR=1e-5
MIN_LR=1e-6
SEQ_LEN=8192
PAD_LEN=8192
PR=fp8
### BASE CONFIG ###

### PARALLEL / BOOL OPTION ###
TP=2
PP=2
CP=1
SP=true
DO=true
FL=true
SFT=false
### PARALLEL / BOOL OPTION ###

### OTHERS ###
AC=false
OPTIMIZER_OFFLOAD=false
SAVE_INTERVAL=100000
DATASET_PATH=/mnt/nfs129/zhongziban/dataset/llama3-dataset/wudao_llama3bpe_content_document
VALID_DATASET_PATH=/mnt/nfs129/zhongziban/dataset/llama3-dataset/wudao_llama3bpe_content_document


# the following two values will not be used when SFT is true
TRAIN_TOKENS=100000000000
WARMUP_TOKENS=100000000
###############################


### OTHERS ###

if [ $FL = true ]; then
    export NVTE_FLASH_ATTN=1 NVTE_FUSED_ATTN=0
elif [ $FL = false ]; then
    export NVTE_FLASH_ATTN=0 NVTE_FUSED_ATTN=1
fi

if [ $MODEL_SIZE = 8B ]; then

NUM_LAYERS=32
HIDDEN_SIZE=4096
NUM_ATTN_HEADS=32
INTERMEDIATE_SIZE=14336
NUM_KEY_VALUE_HEADS=8
MAX_POSITION_EMBEDDINGS=131072

gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"


elif [ $MODEL_SIZE = 70B ]; then

NUM_LAYERS=80
HIDDEN_SIZE=8192
NUM_ATTN_HEADS=64
INTERMEDIATE_SIZE=28672
NUM_KEY_VALUE_HEADS=8
MAX_POSITION_EMBEDDINGS=131072
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

fi

MP_VP=$(( $NUM_LAYERS / $PP / 4 ))

if [ -z ${MP_VP} ]; then
    vp_options=""
else
    vp_options=" \
        --num-layers-per-virtual-pipeline-stage ${MP_VP}"
fi


TP_COMM_OVERLAP=$(( ($TP > 1) ? 1 : 0 ))

comm_overlap_option="\
    --overlap-grad-reduce \
    --overlap-param-gather"

if [ $TP_COMM_OVERLAP -eq 1 ]; then
    comm_overlap_option="\
        --tp-comm-overlap \
        --overlap-grad-reduce \
        --overlap-param-gather"
fi

if [ $AC = full ]; then
    _check=$(( ($NUM_LAYERS / $PP) % ${MP_AC_LAYERS} ))
    if [ $_check != 0 ]; then
        echo "the num layers per pp rank must be a multiple of the recompute layers."
        exit -1
    fi
    activation_checkpoint_options=" \
                    --recompute-method uniform \
            --recompute-num-layers ${MP_AC_LAYERS} \
                    --recompute-granularity full"
elif [ $AC = sel ]; then
    activation_checkpoint_options=" \
        --recompute-activations"
elif [ $AC = none ]; then
    activation_checkpoint_options=" \
    "
elif [ $AC = offload ]; then
    activation_checkpoint_options=" \
                    --cpu-offloading \
                    --cpu-offloading-num-layers ${MP_AC_LAYERS}"
    if [ $TP_COMM_OVERLAP -eq 1 ]; then
        echo "Disable --overlap-grad-reduce and --overlap-param-gather when cpu offloading is on..."
        comm_overlap_option="\
            --tp-comm-overlap"
    else
        echo "Disable --overlap-grad-reduce and --overlap-param-gather when cpu offloading is on..."
        comm_overlap_option=""
    fi
fi

if [ $PR = fp16 ]; then
    pr_options=" \
                    --fp16 \
            --apply-query-key-layer-scaling"
    export NVTE_APPLY_QK_LAYER_SCALING=1
elif [ $PR = bf16 ]; then
    pr_options=" \
        --bf16"
elif [ $PR = fp8 ]; then
    pr_options=" \
        --bf16 \
        --fp8-format hybrid \
        --fp8-amax-compute-algo max \
        --fp8-amax-history-len 1024"
fi

if [ $OPTIMIZER_OFFLOAD != false ] && [ $DO = false ]; then
    echo "Offload optimizer is valid only if \$DO=true"
    DO=true
fi

if [ $DO = true ]; then
    do_options=" \
                    --use-distributed-optimizer"

elif [ $DO = false ]; then
    do_options=" \
                    "
fi

te_options=" \
        --transformer-impl transformer_engine"

if [ $SP = true ] && [ $TP -gt 1 ]; then
    sp_options=" \
                    --sequence-parallel"

elif [ $SP = false ]; then
    sp_options=" \
                    "
fi

if [ $PRETRAIN_CHECKPOINT_PATH != none ]; then
    load_options=" \
            --load $PRETRAIN_CHECKPOINT_PATH"
fi

if [ $OPTIMIZER_OFFLOAD = 'static' ]; then
    offload_option=" \
        --optimizer hybridadam \
        --optimizer-offload-policy static \
        --optimizer-offload-fraction 1.0"
elif [ $OPTIMIZER_OFFLOAD = 'auto' ]; then
    offload_option=" \
        --optimizer hybridadam \
        --optimizer-offload-policy auto"
else
    offload_option=""
fi

if [ $SFT = true ]; then
    TRAIN_ITERS=${23}
    LR_WARMUP_ITERS=${24}
    LR_DECAY_ITERS=$(( ${TRAIN_ITERS} - ${LR_WARMUP_ITERS}))
    PREFIX="finetune-mcore-llama3-1-${MODEL_SIZE}-lr-${LR}-minlr-${MIN_LR}-bs-${BATCH_SIZE}-gbs-${GLOBAL_BATCH_SIZE}-seqlen-${SEQ_LEN}"
    sft_option=" \
         --eod-mask-loss \
         --train-mode finetune"
else
    TRAIN_ITERS=$(( ${TRAIN_TOKENS} / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    LR_WARMUP_ITERS=$(( ${WARMUP_TOKENS}  / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    LR_DECAY_ITERS=$(( ${TRAIN_TOKENS} /  ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    PREFIX="pretrain-mcore-llama3-1-${MODEL_SIZE}-lr-${LR}-minlr-${MIN_LR}-bs-${BATCH_SIZE}-gbs-${GLOBAL_BATCH_SIZE}-seqlen-${SEQ_LEN}"
    sft_option=" \
        --train-mode pretrain"
fi

if [ ${MP_DATASET_TYPE} = "raw" ]; then
    dataset_option=" \
        --train-data-path ${DATASET_PATH} \
        --valid-data-path ${VALID_DATASET_PATH} \
        --dataloader-type cyclic \
        --dataset LLama-SFT-Raw"
else 
    dataset_option=" \
        --data-path ${DATASET_PATH} \
        --split 99,1,0 \
        --dataset LLama-Pretrain-Idxmap"
fi

if [ ${MP_SFT_PACKING} = true ]; then
    packing_options=" \
        --reset-position-ids \
        --no-create-attention-mask-in-dataloader
    "
else
    packing_options=""
fi


##### Prepare logdirs #######
NAME="${PREFIX}-pr-${PR}-tp-${TP}-pp-${PP}-cp-${CP}-ac-${AC}-do-${DO}-sp-${SP}-ti-${TRAIN_ITERS}-wi-${LR_WARMUP_ITERS}"
mkdir -p "${OUTPUT_BASEPATH}/tensorboard/"
mkdir -p "${OUTPUT_BASEPATH}/checkpoint/"
mkdir -p "${OUTPUT_BASEPATH}/log/"
current_time=$(date "+%Y.%m.%d-%H.%M.%S")
TENSORBOARD_DIR="${OUTPUT_BASEPATH}/tensorboard/${NAME}_${current_time}"
mkdir -p ${TENSORBOARD_DIR}
SAVED_PRETRAIN_CHECKPOINT_PATH="${OUTPUT_BASEPATH}/checkpoint/${NAME}"

mkdir -p ${SAVED_PRETRAIN_CHECKPOINT_PATH}
find -L ${PRETRAIN_CHECKPOINT_PATH} -maxdepth 1 -type f -name "*.json" -print0 | xargs -0 cp -t ${SAVED_PRETRAIN_CHECKPOINT_PATH}


megatron_options="  \
        --save ${SAVED_PRETRAIN_CHECKPOINT_PATH} \
        --lr ${LR} \
        --min-lr ${MIN_LR} \
        --lr-decay-style cosine \
        --weight-decay 0.1 \
        --adam-beta1 0.9 \
        --adam-beta2 0.95 \
        --clip-grad 1.0 \
        --init-method-std 0.008 \
        --attention-dropout 0.0 \
        --hidden-dropout 0.0 \
        --lr-decay-iters ${LR_DECAY_ITERS} \
        --lr-warmup-iters ${LR_WARMUP_ITERS} \
        --train-iters ${TRAIN_ITERS} \
        --micro-batch-size ${BATCH_SIZE} \
        --global-batch-size ${GLOBAL_BATCH_SIZE} \
        --num-layers ${NUM_LAYERS} \
        --hidden-size ${HIDDEN_SIZE} \
        --num-attention-heads ${NUM_ATTN_HEADS} \
        --ffn-hidden-size ${INTERMEDIATE_SIZE} \
        --seq-length ${SEQ_LEN} \
        --max-position-embeddings ${MAX_POSITION_EMBEDDINGS} \
        --max-padding-length ${PAD_LEN} \
        --log-interval 1 \
        --log-throughput \
        --eval-interval 10000 \
        --eval-iters 10 \
        --save-interval ${SAVE_INTERVAL} \
        --tensorboard-queue-size 1 \
        --tensorboard-dir ${TENSORBOARD_DIR} \
        --log-timers-to-tensorboard \
        --log-batch-size-to-tensorboard \
        --log-validation-ppl-to-tensorboard \
        --tensor-model-parallel-size ${TP} \
        --pipeline-model-parallel-size ${PP} \
        --context-parallel-size ${CP} \
        --no-load-optim \
        --no-load-rng \
        --num-workers 8 \
        --extra-vocab-size ${EXTRA_VOCAB_SIZE} \
        --patch-tokenizer-type LLama3Tokenizer \
        --swiglu \
        --normalization RMSNorm \
        --norm-epsilon 1e-05 \
        --use-rotary-position-embeddings \
        --position-embedding-type rope \
        --untie-embeddings-and-output-weights \
        --disable-bias-linear \
        --rotary-base 500000 \
        --no-save-optim \
        "

run_cmd="torchrun $DISTRIBUTED_ARGS pretrain_llama.py
 ${megatron_options} ${dataset_option} ${pr_options} ${load_options} ${te_options} ${activation_checkpoint_options} \
 ${do_options} ${sp_options} ${gqa_options} ${offload_option} ${comm_overlap_option} ${sft_option} ${vp_options} ${packing_options} 2>&1 |tee ${OUTPUT_DIR}/logs/training-log-$RANK.${DATETIME}.txt"

echo ${run_cmd} > "${OUTPUT_BASEPATH}/log/run_cmd"
eval ${run_cmd}
set +x
```



### 采样数据-LLama3.1-70B-8机

70B的模型模型层数为80.

`num-layers-per-virtual-pipeline-stage` 取值为 `80/$PP/4`，因此，对于PP=2，则num-layers-per-vitrual-pipleline-stage取10 

对于**LLama3.1 8B**的模型，精度采用bf16和FP8而global batch_size为1024，进行如下的采样：

| precision | TP   | PP   | CP   | mirco-batch-size | AC   | num-layers-per-<br />virtual-pipeline-stage | TFLOPS | elapsed time | memory                                                       |
| --------- | ---- | ---- | ---- | ---------------- | ---- | ------------------------------------------- | ------ | ------------ | ------------------------------------------------------------ |
| bf8       | 2    | 2    | 1    | 1                | none | 10                                          |        |              | OOM                                                          |
| bf8       | 2    | 4    | 1    | 1                | none | 5                                           |        |              | OOM                                                          |
| bf8       | 2    | 8    | 1    | 1                | none | 2                                           |        |              | OOM                                                          |
| bf8       | 2    | 8    | 2    | 1                | none | 2                                           |        |              |                                                              |
| bf8       | 2    | 8    | 4    | 1                | none | 2                                           | 603.6  | 104551.0     | 79030MiB                                                     |
| bf8       | 2    | 8    | 4    | 1                | sel  | 2                                           |        |              |                                                              |
| bf8       | 2    | 8    | 4    | 1                | full | 2                                           |        |              |                                                              |
| bf8       | 2    | 8    | 4    | 1                | none | 5                                           |        |              |                                                              |
|           |      |      |      |                  |      |                                             |        |              |                                                              |
| bf8       | 2    | 8    | 4    | 2                | none | 2                                           |        |              | OOM                                                          |
| bf8       | 2    | 8    | 4    | 2                | sel  | 2                                           |        |              | torch.distributed.elastic.rendezvous.api.RendezvousConnectionError: The connection to the C10d store has failed. See inner exception for details. |
| bf8       | 2    | 8    | 4    | 2                | full | 2                                           |        |              | [7] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '6:7', but store->get('6:7') got error: Connection reset by peer |
| bf8       | 4    | 2    | 8    | 1                | none |                                             |        |              | PP要等于8？CP要<4?，                                         |
|           |      |      |      |                  |      |                                             |        |              |                                                              |
| bf8       | 4    | 8    | 1    | 1                | none | 2                                           | 382.9  | 164800.3     | 80984MiB                                                     |
| bf8       | 4    | 8    | 2    | 1                | none | 2                                           | 630.8  | 100041.5     | 60574MiB                                                     |
| bf8       | 4    | 8    | 2    | 1                | sel  | 2                                           | 594.0  | 106242.2     | 61090MiB                                                     |
| bf8       | 4    | 8    | 2    | 1                | full | 2                                           |        |              | TypeError: te_checkpoint() missing 1 required positional argument: 'packed_seq_params' |
| bf8       | 4    | 8    | 2    | 1                | none | 5                                           |        |              | [7] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '6:7', but store->get('6:7') got error: Connection reset by peer |
| bf8       | 4    | 8    | 2    | 2                | none |                                             |        |              | OOM                                                          |
| bf8       | 4    | 8    | 2    | 2                | full |                                             |        |              | TypeError: te_checkpoint() missing 1 required positional argument: 'packed_seq_params' |
|           |      |      |      |                  |      |                                             |        |              |                                                              |
| bf8       | 8    | 8    | 1    | 1                | none |                                             | 593.3  | 106352.5     | 51032MiB                                                     |
|           |      |      |      |                  |      |                                             |        |              |                                                              |
|           |      |      |      |                  |      |                                             |        |              |                                                              |
|           |      |      |      |                  |      |                                             |        |              |                                                              |
|           |      |      |      |                  |      |                                             |        |              |                                                              |
|           |      |      |      |                  |      |                                             |        |              |                                                              |
|           |      |      |      |                  |      |                                             |        |              |                                                              |
|           |      |      |      |                  |      |                                             |        |              |                                                              |

## Qwen2.5-72B

### 启动脚本-fp8-tp4-pp8-cp2-mbs1-ac-sel-virtual2

```bash
#!/bin/bash
set -e

OUTPUT_BASEPATH="$OUTPUT_DIR"
DATETIME="$(date +'%Y%m%dT%H%M%S')"
# 使用huggingface模型，则会随机进行初始化
PRETRAIN_CHECKPOINT_PATH="$(pwd)"/model/Qwen2.5-72B-tokenizer/

cd Pai-Megatron-Patch/examples/qwen2_5
ENV=dlc
CURRENT_DIR="$( cd "$( dirname "$0" )" && pwd )"
MEGATRON_PATH=$( dirname $( dirname ${CURRENT_DIR}))

LOG_DIR="${OUTPUT_BASEPATH}"

export PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATH}/PAI-Megatron-LM-240718:$PYTHONPATH
export CUDA_DEVICE_MAX_CONNECTIONS=1
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Here are some configs controled by env
if [ -z ${MP_DATASET_TYPE} ];then
    MP_DATASET_TYPE="idxmap"
fi

if [ -z ${MP_AC_LAYERS} ];then
    MP_AC_LAYERS=1
fi

if [ $ENV = dsw ]; then
    export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
    MASTER_ADDR=localhost
    MASTER_PORT=$(shuf -n 1 -i 10000-65535)
    NNODES=1
    NODE_RANK=0
    GPUS_PER_NODE=8
elif [ $ENV = dlc ]; then
    NNODES=${WORLD_SIZE}
    GPUS_PER_NODE=${TQ_GPU_NUM}
fi


if [ -z ${MP_SFT_PACKING} ]; then
    MP_SFT_PACKING=false
fi


DISTRIBUTED_ARGS="--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES \
  --rdzv_id=333 --rdzv_backend=c10d --rdzv_endpoint $MASTER_ADDR:$MASTER_PORT \
  --tee 3 --log_dir ${LOG_DIR}/logs/${DATETIME}"

### BASE CONFIG ###
MODEL_SIZE=72B
BATCH_SIZE=1
GLOBAL_BATCH_SIZE=1024
LR=1e-5
MIN_LR=1e-6
SEQ_LEN=8192
PAD_LEN=8192
PR=fp8
### BASE CONFIG ###

### PARALLEL / BOOL OPTION ###
TP=4
PP=8
CP=2
SP=true
DO=true
FL=true
SFT=false
### PARALLEL / BOOL OPTION ###

### OTHERS ###
AC=sel
OPTIMIZER_OFFLOAD=false
SAVE_INTERVAL=10000
DATASET_PATH=/mnt/nfs129/zhongziban/dataset/qwen-dataset/wudao_qwenbpe_content_document
VALID_DATASET_PATH=/mnt/nfs129/zhongziban/dataset/qwen-dataset/wudao_qwenbpe_content_document


# the following two values will not be used when SFT is true
TRAIN_TOKENS=100000000000
WARMUP_TOKENS=100000000

###############################
### OTHERS ###

if [ $FL = true ]; then
    export NVTE_FLASH_ATTN=1 NVTE_FUSED_ATTN=0
elif [ $FL = false ]; then
    export NVTE_FLASH_ATTN=0 NVTE_FUSED_ATTN=1
fi

if [ $MODEL_SIZE = 0.5B ]; then

NUM_LAYERS=24
HIDDEN_SIZE=896
NUM_ATTN_HEADS=14
INTERMEDIATE_SIZE=4864
NUM_KEY_VALUE_HEADS=2
MAX_POSITION_EMBEDDINGS=32768
EXTRA_VOCAB_SIZE=293
RMS_NORM_EPS=1e-6
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"


tie_option=""

elif [ $MODEL_SIZE = 1.5B ]; then

NUM_LAYERS=28
HIDDEN_SIZE=1536
NUM_ATTN_HEADS=12
INTERMEDIATE_SIZE=8960
NUM_KEY_VALUE_HEADS=2
MAX_POSITION_EMBEDDINGS=32768
EXTRA_VOCAB_SIZE=293
RMS_NORM_EPS=1e-6
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

tie_option=""

elif [ $MODEL_SIZE = 3B ]; then

NUM_LAYERS=36
HIDDEN_SIZE=2048
NUM_ATTN_HEADS=16
INTERMEDIATE_SIZE=11008
NUM_KEY_VALUE_HEADS=2
MAX_POSITION_EMBEDDINGS=32768
EXTRA_VOCAB_SIZE=293
RMS_NORM_EPS=1e-6
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

tie_option=""

elif [ $MODEL_SIZE = 7B ]; then

NUM_LAYERS=28
HIDDEN_SIZE=3584
NUM_ATTN_HEADS=28
INTERMEDIATE_SIZE=18944
NUM_KEY_VALUE_HEADS=4
MAX_POSITION_EMBEDDINGS=131072
EXTRA_VOCAB_SIZE=421
RMS_NORM_EPS=1e-6
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

tie_option=" \
        --untie-embeddings-and-output-weights \
        "

elif [ $MODEL_SIZE = 14B ]; then

NUM_LAYERS=48
HIDDEN_SIZE=5120
NUM_ATTN_HEADS=40
INTERMEDIATE_SIZE=13824
NUM_KEY_VALUE_HEADS=8
MAX_POSITION_EMBEDDINGS=131072
EXTRA_VOCAB_SIZE=421
RMS_NORM_EPS=1e-5
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

tie_option=" \
        --untie-embeddings-and-output-weights \
        "
elif [ $MODEL_SIZE = 32B ]; then

NUM_LAYERS=64
HIDDEN_SIZE=5120
NUM_ATTN_HEADS=40
INTERMEDIATE_SIZE=27648
NUM_KEY_VALUE_HEADS=8
MAX_POSITION_EMBEDDINGS=131072
EXTRA_VOCAB_SIZE=421
RMS_NORM_EPS=1e-5
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

tie_option=" \
        --untie-embeddings-and-output-weights \
        "
elif [ $MODEL_SIZE = 72B ]; then

NUM_LAYERS=80
HIDDEN_SIZE=8192
NUM_ATTN_HEADS=64
INTERMEDIATE_SIZE=29568
NUM_KEY_VALUE_HEADS=8
MAX_POSITION_EMBEDDINGS=131072
EXTRA_VOCAB_SIZE=421
RMS_NORM_EPS=1e-5
gqa_options=" \
                    --group-query-attention \
                    --num-query-groups ${NUM_KEY_VALUE_HEADS}"

tie_option=" \
        --untie-embeddings-and-output-weights \
        "

fi


MP_VP=$(( $NUM_LAYERS / $PP / 4 ))

if [ -z ${MP_VP} ]; then
    vp_options=""
else
    vp_options=" \
        --num-layers-per-virtual-pipeline-stage ${MP_VP}"
fi


TP_COMM_OVERLAP=$(( ($TP > 1) ? 1 : 0 ))
comm_overlap_option="\
    --overlap-grad-reduce \
    --overlap-param-gather"
 

if [ $TP_COMM_OVERLAP -eq 1 ]; then
    comm_overlap_option="\
        --tp-comm-overlap \
        --overlap-grad-reduce \
        --overlap-param-gather"
fi

if [ $AC = full ]; then
    _check=$(( ($NUM_LAYERS / $PP) % ${MP_AC_LAYERS} ))
    if [ $_check != 0 ]; then
        echo "the num layers per pp rank must be a multiple of the recompute layers."
        exit -1
    fi
    activation_checkpoint_options=" \
                    --recompute-method uniform \
            --recompute-num-layers ${MP_AC_LAYERS} \
                    --recompute-granularity full"
elif [ $AC = sel ]; then
    activation_checkpoint_options=" \
        --recompute-activations"
elif [ $AC = none ]; then
    activation_checkpoint_options=" \
    "
elif [ $AC = offload ]; then
    activation_checkpoint_options=" \
                    --cpu-offloading \
                    --cpu-offloading-num-layers ${MP_AC_LAYERS}"
    if [ $TP_COMM_OVERLAP -eq 1 ]; then
        echo "Disable --overlap-grad-reduce and --overlap-param-gather when cpu offloading is on..."
        comm_overlap_option="\
            --tp-comm-overlap"
    else
        echo "Disable --overlap-grad-reduce and --overlap-param-gather when cpu offloading is on..."
        comm_overlap_option=""
    fi
fi

if [ $PR = fp16 ]; then
    pr_options=" \
                    --fp16 \
            --apply-query-key-layer-scaling"
    export NVTE_APPLY_QK_LAYER_SCALING=1
elif [ $PR = bf16 ]; then
    pr_options=" \
        --bf16"
elif [ $PR = fp8 ]; then
    pr_options=" \
        --bf16 \
        --fp8-format hybrid \
        --fp8-amax-compute-algo max \
        --fp8-amax-history-len 1024"
fi

if [ $OPTIMIZER_OFFLOAD != false ] && [ $DO = false ]; then
    echo "Offload optimizer is valid only if \$DO=true"
    DO=true
fi

if [ $DO = true ]; then
    do_options=" \
                    --use-distributed-optimizer"

elif [ $DO = false ]; then
    do_options=" \
                    "
fi

te_options=" \
        --transformer-impl transformer_engine"

if [ $SP = true ] && [ $TP -gt 1 ]; then
    sp_options=" \
                    --sequence-parallel"

elif [ $SP = false ]; then
    sp_options=" \
                    "
fi

if [ $PRETRAIN_CHECKPOINT_PATH != none ]; then
    load_options=" \
            --load $PRETRAIN_CHECKPOINT_PATH"
fi

if [ $OPTIMIZER_OFFLOAD = 'static' ]; then
    offload_option=" \
        --optimizer hybridadam \
        --optimizer-offload-policy static \
        --optimizer-offload-fraction 1.0"
elif [ $OPTIMIZER_OFFLOAD = 'auto' ]; then
    offload_option=" \
        --optimizer hybridadam \
        --optimizer-offload-policy auto"
else
    offload_option=""
fi

if [ $SFT = true ]; then
    TRAIN_ITERS=${23}
    LR_WARMUP_ITERS=${24}
    LR_DECAY_ITERS=$(( ${TRAIN_ITERS} - ${LR_WARMUP_ITERS}))
    PREFIX="finetune-mcore-qwen2.5-${MODEL_SIZE}-lr-${LR}-minlr-${MIN_LR}-bs-${BATCH_SIZE}-gbs-${GLOBAL_BATCH_SIZE}-seqlen-${SEQ_LEN}"
    sft_option=" \
         --eod-mask-loss \
         --train-mode finetune"
else
    TRAIN_ITERS=$(( ${TRAIN_TOKENS} / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    LR_WARMUP_ITERS=$(( ${WARMUP_TOKENS}  / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    LR_DECAY_ITERS=$(( ${TRAIN_TOKENS} /  ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))
    PREFIX="pretrain-mcore-qwen2.5-${MODEL_SIZE}-lr-${LR}-minlr-${MIN_LR}-bs-${BATCH_SIZE}-gbs-${GLOBAL_BATCH_SIZE}-seqlen-${SEQ_LEN}"
    sft_option=" \
        --train-mode pretrain"
fi

if [ ${MP_DATASET_TYPE} = "raw" ]; then
    dataset_option=" \
        --train-data-path ${DATASET_PATH} \
        --valid-data-path ${VALID_DATASET_PATH} \
        --dataloader-type cyclic \
        --dataset LLama-SFT-Raw"
else 
    dataset_option=" \
        --data-path ${DATASET_PATH} \
        --split 99,1,0 \
        --dataset LLama-Pretrain-Idxmap"
fi

if [ ${MP_SFT_PACKING} = true ]; then
    packing_options=" \
        --reset-position-ids \
        --no-create-attention-mask-in-dataloader
    "
else
    packing_options=""
fi

##### Prepare logdirs #######
NAME="${PREFIX}-pr-${PR}-tp-${TP}-pp-${PP}-cp-${CP}-ac-${AC}-do-${DO}-sp-${SP}-ti-${TRAIN_ITERS}-wi-${LR_WARMUP_ITERS}"
mkdir -p "${OUTPUT_BASEPATH}/tensorboard/"
mkdir -p "${OUTPUT_BASEPATH}/checkpoint/"
mkdir -p "${OUTPUT_BASEPATH}/log/"
current_time=$(date "+%Y.%m.%d-%H.%M.%S")
TENSORBOARD_DIR="${OUTPUT_BASEPATH}/tensorboard/${NAME}_${current_time}"
mkdir -p ${TENSORBOARD_DIR}
SAVED_PRETRAIN_CHECKPOINT_PATH="${OUTPUT_BASEPATH}/checkpoint/${NAME}"

mkdir -p ${SAVED_PRETRAIN_CHECKPOINT_PATH}
find -L ${PRETRAIN_CHECKPOINT_PATH} -maxdepth 1 -type f -name "*.json" -print0 | xargs -0 cp -t ${SAVED_PRETRAIN_CHECKPOINT_PATH}
find -L ${PRETRAIN_CHECKPOINT_PATH} -maxdepth 1 -type f -name "merges.txt" -print0 | xargs -0 cp -t ${SAVED_PRETRAIN_CHECKPOINT_PATH}

megatron_options="  \
        --save ${SAVED_PRETRAIN_CHECKPOINT_PATH} \
        --lr ${LR} \
        --min-lr ${MIN_LR} \
        --lr-decay-style cosine \
        --weight-decay 0.1 \
        --adam-beta1 0.9 \
        --adam-beta2 0.95 \
        --clip-grad 1.0 \
        --init-method-std 0.008 \
        --attention-dropout 0.0 \
        --hidden-dropout 0.0 \
        --lr-decay-iters ${LR_DECAY_ITERS} \
        --lr-warmup-iters ${LR_WARMUP_ITERS} \
        --train-iters ${TRAIN_ITERS} \
        --micro-batch-size ${BATCH_SIZE} \
        --global-batch-size ${GLOBAL_BATCH_SIZE} \
        --num-layers ${NUM_LAYERS} \
        --hidden-size ${HIDDEN_SIZE} \
        --num-attention-heads ${NUM_ATTN_HEADS} \
        --ffn-hidden-size ${INTERMEDIATE_SIZE} \
        --seq-length ${SEQ_LEN} \
        --max-position-embeddings ${MAX_POSITION_EMBEDDINGS} \
        --max-padding-length ${PAD_LEN} \
        --log-interval 1 \
        --log-throughput \
        --eval-interval 10000 \
        --eval-iters 10 \
        --save-interval ${SAVE_INTERVAL} \
        --tensorboard-queue-size 1 \
        --tensorboard-dir ${TENSORBOARD_DIR} \
        --log-timers-to-tensorboard \
        --log-batch-size-to-tensorboard \
        --log-validation-ppl-to-tensorboard \
        --tensor-model-parallel-size ${TP} \
        --pipeline-model-parallel-size ${PP} \
        --context-parallel-size ${CP} \
        --no-load-optim \
        --no-load-rng \
        --num-workers 8 \
        --extra-vocab-size ${EXTRA_VOCAB_SIZE} \
        --patch-tokenizer-type Qwen2Tokenizer \
        --swiglu \
        --normalization RMSNorm \
        --norm-epsilon ${RMS_NORM_EPS} \
        --use-rotary-position-embeddings \
        --position-embedding-type rope \
        --disable-bias-linear \
        --add-qkv-bias \
        --rotary-percent 1.0 \
        --rotary-base 1000000 \
        --rotary-seq-len-interpolation-factor 1 \
        --no-save-optim \
        "

run_cmd="torchrun $DISTRIBUTED_ARGS ../qwen2/pretrain_qwen.py
 ${megatron_options} ${dataset_option} ${pr_options} ${load_options} ${te_options} ${activation_checkpoint_options} \
 ${do_options} ${sp_options} ${gqa_options} ${offload_option} ${comm_overlap_option} ${sft_option}  ${tie_option} ${vp_options} ${packing_options} 2>&1 |tee ${OUTPUT_DIR}/logs/training-log-$RANK.${DATETIME}.txt"

filename="${OUTPUT_BASEPATH}/log/torchrun_$(date +"%Y-%m-%d_%H-%M-%S").log"
echo ${run_cmd} >"$filename"
eval ${run_cmd}
set +x
```









### 采样数据Qwen-72B-8m

|      | precision | TP   | PP   | CP   | mirco-batch-size | AC   | num-layers-per-<br />virtual-pipeline-stage | TFLOPS | elapsed time | memory                                                       |
| ---- | --------- | ---- | ---- | ---- | ---------------- | ---- | ------------------------------------------- | ------ | ------------ | ------------------------------------------------------------ |
|      | bf8       | 1    | 8    | 8    |                  |      |                                             |        |              |                                                              |
|      |           |      |      |      |                  |      |                                             |        |              |                                                              |
|      | bf8       | 2    | 4    | 8    | 1                |      |                                             |        |              | OOM                                                          |
|      |           |      |      |      |                  |      |                                             |        |              |                                                              |
|      | bf8       | 2    | 8    | 4    | 1                | none | 2                                           |        |              | OOM                                                          |
|      | bf8       | 2    | 8    | 4    | 1                | sel  | 2                                           |        |              |                                                              |
|      | bf8       | 2    | 8    | 4    | 1                | full | 2                                           |        |              |                                                              |
|      | bf8       | 2    | 8    | 4    | 1                | none | 5                                           |        |              |                                                              |
|      |           |      |      |      |                  |      |                                             |        |              |                                                              |
|      | bf8       | 2    | 8    | 4    | 2                | none | 2                                           |        |              |                                                              |
|      | bf8       | 2    | 8    | 4    | 2                | sel  | 2                                           |        |              |                                                              |
|      | bf8       | 2    | 8    | 4    | 2                | full | 2                                           |        |              |                                                              |
|      | bf8       | 4    | 2    | 8    | 1                | none |                                             |        |              | RuntimeError: CUDA error: an illegal memory access was encountered |
|      |           |      |      |      |                  |      |                                             |        |              |                                                              |
|      | bf8       | 4    | 4    | 8    | 1                |      |                                             |        |              |                                                              |
|      |           |      |      |      |                  |      |                                             |        |              |                                                              |
|      | bf8       | 4    | 8    | 1    | 1                | none | 2                                           |        |              |                                                              |
| √    | bf8       | 4    | 8    | 2    | 1                | none | 2                                           | 619.6  | 104322.3     | 62494MiB                                                     |
| √    | bf8       | 4    | 8    | 2    | 1                | sel  | 2                                           | 588.9  | 109770.9     | 63130MiB                                                     |
|      | bf8       | 4    | 8    | 2    | 1                | full | 2                                           |        |              |                                                              |
|      | bf8       | 4    | 8    | 2    | 1                | none | 5                                           |        |              |                                                              |
|      | bf8       | 4    | 8    | 2    | 2                | none |                                             |        |              |                                                              |
|      | bf8       | 4    | 8    | 2    | 2                | full |                                             |        |              |                                                              |
|      |           |      |      |      |                  |      |                                             |        |              |                                                              |
|      | bf8       | 4    | 8    | 4    | 1                | none |                                             |        |              |                                                              |
|      |           |      |      |      |                  |      |                                             |        |              |                                                              |
|      | bf8       | 8    | 8    | 1    | 1                | none |                                             |        |              |                                                              |
|      |           |      |      |      |                  |      |                                             |        |              |                                                              |
|      |           |      |      |      |                  |      |                                             |        |              |                                                              |
|      |           |      |      |      |                  |      |                                             |        |              |                                                              |
|      |           |      |      |      |                  |      |                                             |        |              |                                                              |
|      |           |      |      |      |                  |      |                                             |        |              |                                                              |
|      |           |      |      |      |                  |      |                                             |        |              |                                                              |
|      |           |      |      |      |                  |      |                                             |        |              |                                                              |

# 总结

